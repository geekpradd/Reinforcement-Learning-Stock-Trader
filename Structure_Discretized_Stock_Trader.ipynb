{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Structure_Discretized_Stock_Trader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMrXYV0PyooSK+0PnZxVZoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekpradd/Reinforcement-Learning-Stock-Trader/blob/master/Structure_Discretized_Stock_Trader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRLYZIX8iJke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60595d70-d11f-48a8-d45a-a536ec7e2be8"
      },
      "source": [
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pickle \n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgVtrpBiMsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size, timesteps):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        final = []\n",
        "        for i in indices:\n",
        "            if i == self.index:\n",
        "                i = random.sample(range(self.size), 1)\n",
        "            final.append(np.array(buffer[i:min(self.max_size, i+timesteps)]))\n",
        "        return final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93KQJaniMzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, num_actions):\n",
        "        self.model = keras.Sequential()\n",
        "        self.model.add(keras.layers.Dense(256, activation = 'relu', input_shape = (4, )))\n",
        "        self.model.add(keras.layers.Dropout(0.5))\n",
        "        self.model.add(keras.layers.Dense(128, activation = 'relu'))\n",
        "        self.model.add(keras.layers.LSTM(64, initial_states = [np.zeros(()), np.zeros(())]))\n",
        "        self.model.add(keras.layers.Dense(num_actions))\n",
        "        self.model.compile(optimizer = keras.optimizers.Adam(), loss = 'mse', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo6r3fU-iM2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, agent_params):\n",
        "        self.discount = agent_params['discount']\n",
        "        self.epsilon = agent_params['epsilon']\n",
        "        self.num_actions = agent_params['num_actions']\n",
        "        self.exp_size = agent_params['exp_size']\n",
        "        self.timesteps = agent_params['timesteps']\n",
        "        self.merge = agent_params['merge_threshold']\n",
        "        self.batch_size = agent_params['batch_size']\n",
        "        self.last_action = None\n",
        "        self.last_state = None\n",
        "        self.exp = ReplayMemory(self.exp_size)\n",
        "        self.target_model = Model(self.num_actions)\n",
        "        self.value_model = Model(self.num_actions)\n",
        "        self.num_updates = 0\n",
        "        self.num_steps = 0\n",
        "        self.num_merge = 0\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "\n",
        "    def run(self):\n",
        "        updates = min(self.batch_size, self.exp.size)\n",
        "        self.num_updates += updates\n",
        "        batch = self.exp.sample(updates)\n",
        "\n",
        "        input = [x[0] for x in batch]\n",
        "        inp = [x[3] for x in batch]\n",
        "        x_train = np.concatenate(input, axis = 0)\n",
        "        y_train = self.value_model.model.predict(x_train)\n",
        "        x_target = np.concatenate(inp, axis = 0)\n",
        "        y_target = self.target_model.model.predict(x_target)\n",
        "\n",
        "        for count, memory in enumerate(batch):\n",
        "            last_state, last_action, reward, state, terminal = memory\n",
        "            y_train[count][last_action] = reward\n",
        "            if not terminal:\n",
        "                y_train[count][last_action] += self.discount*np.amax(y_target[count])\n",
        "\n",
        "        self.value_model.model.fit(x_train, y_train, verbose = 0, epochs = 1)\n",
        "\n",
        "        if self.num_updates > self.merge:\n",
        "            self.merge_model()\n",
        "\n",
        "    def merge_model(self):\n",
        "        self.target_model.model.set_weights(self.value_model.model.get_weights())\n",
        "        self.num_merge += 1\n",
        "        self.num_updates = 0\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > 0.1:\n",
        "            self.epsilon -= 0.9/1000000\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.value_model.model.predict(state)[0])\n",
        "\n",
        "        self.num_steps += 1\n",
        "        self.epsilon_decay()\n",
        "        return action\n",
        "\n",
        "    def agent_start(self, state):\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, state):\n",
        "        memory = (self.last_state, self.last_action, reward, state, 0)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.last_action = self.choose_action(state)\n",
        "        self.last_state = state\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        memory = (self.last_state, self.last_action, reward, np.zeros((1, 4)), 1)\n",
        "        self.exp.append(memory)\n",
        "        self.run()\n",
        "        self.num_games += 1\n",
        "\n",
        "    def plot(self, states):\n",
        "        y = np.mean(np.max(self.value_model.model.predict(states), axis = 1))\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63pBaIbLiM5r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b4d2c98c-0566-418a-c628-75124fa6f227"
      },
      "source": [
        "agent_params = {\n",
        "    'discount' :0.99,\n",
        "    'epsilon' :1.0,\n",
        "    'num_actions' : 200,\n",
        "    'exp_size': 600000,\n",
        "    'timesteps' : 10,\n",
        "    'merge_threshold' : 320000,\n",
        "    'batch_size' : 32,\n",
        "}\n",
        "agent = Agent(agent_params)\n",
        "timesteps = []\n",
        "q_value = []\n",
        "avg_reward = []"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-36e875470e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m'batch_size'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-6317d65f3f92>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, agent_params)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-e34b9dd890a7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_actions)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                                    \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m                                    \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m                                    **kwargs)\n\u001b[0m\u001b[1;32m   2239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                              \u001b[0;34m'(tuple of integers, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                              'one integer per RNN state).')\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'initial_state')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W00pNxJOiM9F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "b91e5184-da13-4980-f5a4-7b3d57c8248c"
      },
      "source": [
        "# env = "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ed2d3814a34b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    env =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3djRaXikIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent, env):\n",
        "    timesteps = 4000\n",
        "    total = 0\n",
        "    sum = 0\n",
        "    for _ in range(1, timesteps+1):\n",
        "        action = agent.agent_start(env.reset().reshape(1, 4))\n",
        "        observations, reward, done, info = env.step([action])\n",
        "        total += reward\n",
        "        sum += reward\n",
        "\n",
        "        while not done:\n",
        "            observations, reward, done, info = env.step([agent.agent_step(reward, observations.reshape((1, 128)))])\n",
        "            total += reward\n",
        "            sum += reward\n",
        "\n",
        "        agent.agent_end(reward)\n",
        "\n",
        "        if agent.num_games % 50 == 0:\n",
        "            episodes.append(agent.num_games)\n",
        "            q_value.append(agent.plot(test_states))\n",
        "            avg_reward.append(total/50)\n",
        "            total = 0\n",
        "\n",
        "        if agent.num_games % 100 == 0:\n",
        "            print('Games = {}, Steps = {}, Reward = {}'.format(_, agent.num_steps, sum/100))\n",
        "            sum = 0\n",
        "\n",
        "        if agent.num_games % 400 == 0:\n",
        "            with open('/content/drive/My Drive/DQN/episodes2.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/DQN/q_value2.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/DQN/avg_reward2.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/DQN/test_states2.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "            with open('/content/drive/My Drive/DQN/agent2.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n",
        "        elif agent.num_games % 200 == 0:\n",
        "            with open('/content/drive/My Drive/DQN/episodes3.pkl', 'wb') as f:\n",
        "                pickle.dump(episodes, f)\n",
        "            with open('/content/drive/My Drive/DQN/q_value3.pkl', 'wb') as f:\n",
        "                pickle.dump(q_value, f)\n",
        "            with open('/content/drive/My Drive/DQN/avg_reward3.pkl', 'wb') as f:\n",
        "                pickle.dump(avg_reward, f)\n",
        "            with open('/content/drive/My Drive/DQN/test_states3.pkl', 'wb') as f:\n",
        "                pickle.dump(test_states, f)\n",
        "            with open('/content/drive/My Drive/DQN/agent3.pkl', 'wb') as f:\n",
        "                pickle.dump(agent, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}