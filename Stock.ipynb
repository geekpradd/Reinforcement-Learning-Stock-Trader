{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1miSidLFyxZ4_hMsyMTfqGascj5Z4oeXk",
      "authorship_tag": "ABX9TyOeybev8ioN6dHWJVMd3/No",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekpradd/Reinforcement-Learning-Stock-Trader/blob/master/Stock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9v6LgY5-xd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "from gym import spaces\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Lambda\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow import convert_to_tensor as convert\n",
        "# COLAB = False\n",
        "# if not COLAB:\n",
        "#     import os\n",
        "#     os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "path_base = '/content/drive/My Drive/Stock/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQRD0aSl-15V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StockEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, df, params, train = True):\n",
        "        super(StockEnv,self).__init__()\n",
        "        \n",
        "        self.num_stocks = params['num_stocks']\n",
        "        self.min_brokerage = params['min_brokerage']\n",
        "        self.brokerage_rate = params['brokerage_rate']\n",
        "        self.balance_normal = params['balance_normal']\n",
        "        self.shares_normal = params['shares_normal']\n",
        "        self.volume_normal = params['volume_normal']\n",
        "        self.dfs = df\n",
        "        self.state_dimensions = self.num_stocks*5+1\n",
        "        self.train = train\n",
        "\n",
        "        assert len(df) == self.num_stocks, \"Size of database not equal to number of stocks\"\n",
        "\n",
        "        self.max_steps = min([len(d.loc[:,'Open']) for d in self.dfs])\n",
        "        self.action_space = spaces.Box(low = -1, high = 1, shape =  (1, self.num_stocks), dtype = np.float32)\n",
        "        self.observation_space = spaces.Box(low = 0, high = 1, shape = (1, self.state_dimensions), dtype = np.float32)\n",
        "\n",
        "    def reset(self, intial_balance = 10000, shares_held = None):\n",
        "\n",
        "        if self.train:\n",
        "            self.current_step = np.random.randint(0, self.max_steps)\n",
        "        else:\n",
        "            self.current_step = 0\n",
        "        self.balance = intial_balance\n",
        "        self.shares_held = shares_held\n",
        "        if self.shares_held is None:\n",
        "            self.shares_held = np.zeros((1, self.num_stocks))\n",
        "        self.current_price = self.get_price()\n",
        "        self.highest_price = 0\n",
        "        self.net_worth = self.balance + np.sum(self.shares_held*self.current_price)\n",
        "        self.initial_worth = self.net_worth\n",
        "        self.max_net_worth = self.net_worth\n",
        "        self.set_high()\n",
        "        self.done = False\n",
        "        self.frame = np.zeros((1, self.state_dimensions))\n",
        "        self.info = {\n",
        "            'current_step' : self.current_step,\n",
        "            'current_price': self.current_price,\n",
        "            'highest_price': self.highest_price,\n",
        "            'net_worth' : self.net_worth,\n",
        "            'max_net_worth': self.max_net_worth,\n",
        "            'shares_held' : self.shares_held,\n",
        "        }\n",
        "        return  self.observe()\n",
        "        \n",
        "    def get_price(self):\n",
        "        return np.array([np.random.uniform(df.loc[self.current_step,\"Low\"], df.loc[self.current_step,\"High\"]) for df in self.dfs]).reshape((1, self.num_stocks))\n",
        "      \n",
        "    def set_high(self):\n",
        "        high = np.array([df.loc[self.current_step, 'High'] for df in self.dfs]).reshape((1, self.num_stocks))\n",
        "        self.highest_price = np.maximum(self.highest_price, high)\n",
        "    \n",
        "    def validate(self, action):\n",
        "        sum = 0\n",
        "        for i in range(self.num_stocks):\n",
        "            if action[i] < 0:\n",
        "                if self.shares_held[0][i] < -action[i]:\n",
        "                    return False, 0\n",
        "            sum -= self.broke(self.current_price[0][i]*abs(action[i]))\n",
        "        \n",
        "        sum -= np.sum(self.current_price[0]*action)\n",
        "        if sum + self.balance < 0:\n",
        "            return False, 0\n",
        "        return True, sum\n",
        "\n",
        "    def observe(self):\n",
        "        for i in range(self.num_stocks):\n",
        "            self.frame[0, 4*i:4*i+4] = np.array([self.dfs[i].loc[self.current_step,'Open'],self.dfs[i].loc[self.current_step,'High'],self.dfs[i].loc[self.current_step,'Low'],self.dfs[i].loc[self.current_step,'Close']])/self.highest_price[0, i]\n",
        "        self.frame[0, self.num_stocks*4:self.num_stocks*5] = self.shares_held/self.shares_normal\n",
        "        self.frame[0, 5*self.num_stocks] = self.balance/self.balance_normal\n",
        "        self.info = {\n",
        "            'current_step' : self.current_step,\n",
        "            'current_price': self.current_price,\n",
        "            'highest_price': self.highest_price,\n",
        "            'net_worth' : self.net_worth,\n",
        "            'max_net_worth': self.max_net_worth,\n",
        "            'shares_held' : self.shares_held\n",
        "        }\n",
        "        return self.frame, self.info\n",
        "        \n",
        "    def broke(self, amount):\n",
        "        return max(amount * self.brokerage_rate, self.min_brokerage)\n",
        "    \n",
        "    def update(self, reward):\n",
        "        self.net_worth += reward\n",
        "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
        "    \n",
        "    def take_action(self, action):\n",
        "        action *= self.shares_normal\n",
        "        self.current_price = self.get_price()\n",
        "        validation = self.validate(action)\n",
        "        if not validation[0]:\n",
        "            return -5000, False\n",
        "        self.set_high()\n",
        "        self.balance += validation[1]\n",
        "        self.shares_held += action\n",
        "        reward = self.balance + np.sum(self.shares_held * self.current_price) - self.net_worth\n",
        "        self.update(reward)\n",
        "        return reward, True\n",
        "            \n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.max_steps or self.done:\n",
        "            self.done = True\n",
        "            return np.zeros((1, self.state_dimensions)), 0, self.done, self.info\n",
        "        reward, status = self.take_action(action)\n",
        "        self.done = self.net_worth <= self.initial_worth*0.1\n",
        "        obs, info = self.observe()\n",
        "        return obs, reward, self.done, info\n",
        "    \n",
        "    def render(self, mode='human', close = False):\n",
        "        profit = self.net_worth - self.initial_worth\n",
        "        print('Step: {}'.format(self.current_step))\n",
        "        print('Net Worth: {}'.format(self.net_worth))\n",
        "        print('Profit: {}'.format(profit))\n",
        "        \n",
        "def create_stock_env(locations, train=True):\n",
        "    dfs = [pd.read_csv(location).sort_values('Date') for location in locations]\n",
        "    params = {\n",
        "        'num_stocks' : 2,\n",
        "        'min_brokerage' : 30.0,\n",
        "        'brokerage_rate' : 0.001,\n",
        "        'balance_normal' : 1000000,\n",
        "        'shares_normal' : 10000,\n",
        "        'volume_normal' : 2147483647,\n",
        "    }\n",
        "    return StockEnv(dfs, params, train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H1G_n_8XSHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]\n",
        "        \n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzNRNhM6-197",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, params):\n",
        "        self.output_range = params[\"output_range\"]\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.actor = self.build_model()\n",
        "        \n",
        "    def build_model(self):\n",
        "        inputs = Input(shape=(self.state_dimensions, ))\n",
        "        x = Dense(60, activation = 'relu')(inputs)\n",
        "        x = Dense(16, activation = 'relu')(x)\n",
        "        x = Dense(self.action_dimensions, activation = 'tanh')(x)\n",
        "        output = Lambda(lambda x: x*self.output_range)(x)\n",
        "        model = keras.Model(inputs = inputs, outputs = output)\n",
        "        return model\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        return self.actor(convert(state))\n",
        "\n",
        "    def save(self):\n",
        "        self.actor.save_weights(path_base + 'actor.h5')\n",
        "    \n",
        "    def load(self):\n",
        "        self.actor.load_weights(path_base + 'actor.h5')\n",
        "        print('Successfully Loaded')\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, params):\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.optimizer = params[\"critic_optimizer\"]\n",
        "        self.tau = params['tau']\n",
        "        self.critic_online = self.build_model()\n",
        "        self.critic_target = self.build_model()\n",
        "        self.critic_online.set_weights(self.critic_target.get_weights())\n",
        "\n",
        "    def build_model(self):\n",
        "        input_a = Input(shape = (self.state_dimensions, ))\n",
        "        input_b = Input(shape = (self.action_dimensions, ))\n",
        "        input = Concatenate(axis = -1)([input_a, input_b])\n",
        "        x = Dense(60, activation = 'relu')(input)\n",
        "        x = Dense(16, activation = 'relu')(x)\n",
        "        output = Dense(1)(x)\n",
        "        model = keras.Model(inputs=[input_a, input_b], outputs = output)\n",
        "        model.compile(loss='mse', optimizer = keras.optimizers.Adam(learning_rate = 0.001))\n",
        "        # model.summary()\n",
        "        return model\n",
        "\n",
        "    def save(self):\n",
        "        self.critic_online.save(path_base + 'critic_online.h5')\n",
        "        self.critic_target.save(path_base + 'critic_target.h5')\n",
        "\n",
        "    def load(self):\n",
        "        self.critic_online = keras.models.load_model(path_base + 'critic_online.h5')\n",
        "        self.critic_target = keras.models.load_model(path_base + 'critic_target.h5')\n",
        "\n",
        "    def get_qvalues(self, state_array, action_array, online=True):\n",
        "        if online:\n",
        "            return self.critic_online([convert(state_array), convert(action_array)])\n",
        "        else:\n",
        "            return self.critic_target([convert(state_array), convert(action_array)])\n",
        "\n",
        "    def call(self, state_tensor, action_tensor):\n",
        "        return self.critic_online([state_tensor, action_tensor])\n",
        "    \n",
        "    def merge(self):\n",
        "        self.critic_target.set_weights(self.tau*np.array(self.critic_online.get_weights())\n",
        "                                                                    + (1-self.tau)*np.array(self.critic_target.get_weights()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCOxwbuV-2CH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, params, train = True, resume = True):\n",
        "        self.train = train\n",
        "        self.actor = Actor(params)\n",
        "        self.critic = Critic(params)\n",
        "        self.buffer = ReplayMemory(params[\"buffer_size\"])\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.discount = params[\"discount\"]\n",
        "        self.action_range = params[\"output_range\"]\n",
        "        self.save_frequency = params[\"save_frequency\"]\n",
        "        self.batch_size = params[\"batch_size\"]\n",
        "        self.optimizer = params[\"actor_optimizer\"]\n",
        "        self.num_steps = 0\n",
        "        self.noise_func =  OrnsteinUhlenbeckActionNoise(mu=np.zeros(params[\"action_dimensions\"]))\n",
        "        if resume:\n",
        "            self.load()\n",
        "        \n",
        "    def agent_start(self, observation):\n",
        "        observation = np.reshape(observation, (1, self.state_dimensions))\n",
        "        action = self.actor.get_action(observation)[0]\n",
        "        if self.train:\n",
        "            action = self.clip_action(action + self.noise_func())\n",
        "        else:\n",
        "            action = self.clip_action(action)\n",
        "\n",
        "        self.prev_state = observation\n",
        "        self.prev_action = action\n",
        "        return action\n",
        "\n",
        "    def clip_action(self, action):\n",
        "        action = np.clip(action, -self.action_range, self.action_range)\n",
        "        return action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        observation = np.reshape(observation, (1, self.state_dimensions))\n",
        "        if self.train:\n",
        "            replay = (self.prev_state, self.prev_action, reward, observation)\n",
        "            self.buffer.append(replay)\n",
        "        action = self.actor.get_action(observation)[0]\n",
        "        if self.train:\n",
        "            action = self.clip_action(action + self.noise_func())\n",
        "            self.run()\n",
        "        else:\n",
        "            action = self.clip_action(action)\n",
        "        self.prev_action = action\n",
        "        self.prev_state = observation\n",
        "        return self.prev_action \n",
        "    \n",
        "    def save(self):\n",
        "        self.actor.save()\n",
        "        self.critic.save()\n",
        "\n",
        "    def load(self):\n",
        "        self.actor.load()\n",
        "        self.critic.load()\n",
        "    \n",
        "    def run(self):\n",
        "        self.num_steps += 1\n",
        "        size = min(self.batch_size, self.buffer.size)\n",
        "        batch = self.buffer.sample(size)\n",
        "\n",
        "        prev_states = np.array([x[0] for x in batch]).reshape((-1, self.state_dimensions))\n",
        "        prev_actions = np.array([x[1] for x in batch]).reshape((-1, self.action_dimensions))\n",
        "        rewards = np.array([x[2] for x in batch]).reshape((-1, 1))\n",
        "        states = np.array([x[3] for x in batch]).reshape((-1, self.state_dimensions))\n",
        "\n",
        "        actions = self.actor.get_action(states)\n",
        "        q_values = self.critic.get_qvalues(states, actions, False)\n",
        "        q_values += self.discount*rewards\n",
        "        self.critic.critic_online.fit([states, actions], q_values, epochs = 1, verbose=0)\n",
        "\n",
        "        prev_state_tensor = convert(prev_states)\n",
        "        prev_action_tensor = convert(prev_actions)\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(prev_action_tensor)\n",
        "            value = self.critic.call(prev_state_tensor, prev_action_tensor)\n",
        "            action = self.actor.actor(prev_state_tensor)\n",
        "        gradient = -tape.gradient(value, prev_action_tensor)\n",
        "        gradient = tf.cast(gradient, tf.float32)\n",
        "        gradient_actor = tape.gradient(action, self.actor.actor.trainable_weights, gradient)\n",
        "        gradient_actor = list(np.array(gradient_actor)/size)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradient_actor, self.actor.actor.trainable_weights))\n",
        "        self.critic.merge()\n",
        "\n",
        "        if self.num_steps % self.save_frequency == 0:\n",
        "            self.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GTScpzk-2GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AGENT_PARAMS = {\n",
        "\t\"output_range\": 1,\n",
        "\t\"state_dimensions\": 11,\n",
        "\t\"action_dimensions\": 2,\n",
        "\t\"critic_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "\t\"actor_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "\t\"batch_size\": 64,\n",
        "\t\"buffer_size\":100000,\n",
        "\t\"discount\": 0.99,\n",
        "\t\"tau\": 0.001,\n",
        "\t\"save_frequency\": 5000\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41Cx9flY-2J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = ['/content/drive/My Drive/AAPL.csv','/content/drive/My Drive/MSFT.csv']\n",
        "env = create_stock_env(files)\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "agent = Agent(AGENT_PARAMS, resume = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyCI3wzL-2NI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ITERATIONS = 5000\n",
        "profit = np.zeros((10,ITERATIONS))\n",
        "action = np.zeros((10,ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "shares = np.zeros((10,ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "for iter in range(1):\n",
        "    prev_profit = 0\n",
        "    y, info = env.reset()\n",
        "    action[iter, 0,:] = agent.agent_start(y)\n",
        "    for i in tqdm_notebook(range(ITERATIONS)):\n",
        "        shares[iter,i,:] = info['shares_held']\n",
        "        y, reward, done, info = env.step(action[iter,i])\n",
        "        if done:\n",
        "            break\n",
        "        action[iter,i+1,:] = agent.agent_step(reward, y)\n",
        "        profit[iter][i] += reward + prev_profit\n",
        "        prev_profit += reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfRZEK4e-2Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test\n",
        "env_t = create_stock_env(files,0)\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "agent_t = Agent(AGENT_PARAMS, True)\n",
        "ITERATIONS = 5000\n",
        "profit_t = np.zeros((1,ITERATIONS))\n",
        "action_t = np.zeros((1,ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "shares_t = np.zeros((1,ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "for iter in range(1):\n",
        "    prev_profit = 0\n",
        "    y, info = env_t.reset()\n",
        "    action_t[iter,0,:] = agent_t.agent_start(y)\n",
        "    for i in tqdm_notebook(range(ITERATIONS)):\n",
        "        shares_t[iter,i,:] = info['shares_held']\n",
        "        y, reward, done, info = env_t.step(action_t[iter,i])\n",
        "        action_t[iter,i+1,:] = agent_t.agent_step(reward, y)\n",
        "        profit_t[iter][i] += reward + prev_profit\n",
        "        prev_profit += reward\n",
        "        if done:\n",
        "            print (\"Terminated because broke\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M898siF5ZLy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(profit[0,:])\n",
        "# print((env.dfs[0].loc[5000, \"Open\"]*10000)/env.dfs[0].loc[0, \"Open\"])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hDwiyKMas7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(action[0,:,0])\n",
        "plt.legend(['action '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCgpMEBKazFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "plt.plot(shares[9,:,0])\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqe0hyPea51Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(shares[9,:,1])\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxvlbHEta8mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(profit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0Wx2VchbDzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(min(profit),max(profit),profit[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3KZTtDPbEH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = create_stock_env(files,False)\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "agent = Agent(AGENT_PARAMS, True)\n",
        "ITERATIONS = 5000\n",
        "profit = np.zeros(ITERATIONS)\n",
        "action = np.zeros((ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "shares = np.zeros((ITERATIONS+1,AGENT_PARAMS[\"action_dimensions\"]))\n",
        "balance = np.zeros(ITERATIONS+1)\n",
        "prev_profit = 0\n",
        "y, info = env.reset()\n",
        "action[0,:] = agent.agent_start(y)\n",
        "for i in tqdm(range(ITERATIONS)):\n",
        "    shares[i,:] = info['shares_held']\n",
        "    balance[i] =  info['balance']\n",
        "    y, reward, done, info = env.step(action[i])\n",
        "    action[i+1,:] = agent.agent_step(reward, y)\n",
        "    profit[i] += reward + prev_profit\n",
        "    prev_profit += reward\n",
        "    if done:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOsN5qIAbUcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(action[:,1])\n",
        "plt.legend(['action '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsZouWUTbXZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(action[:,0])\n",
        "plt.legend(['action '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lpyp-B1UbaAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure(figsize = [15, 10])\n",
        "#plt.plot(profit[0])\n",
        "plt.plot(profit)\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzH02VIvbeUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure(figsize = [15, 10])\n",
        "plt.plot(shares[:,1])\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJqu2oytbgIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = [15, 10])\n",
        "plt.plot(shares[:,0])\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oveUGfnNbmIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.figure(figsize = [15, 10])\n",
        "plt.plot(balance)\n",
        "plt.legend(['iteration '+str(i+1) for i in range(10)])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YxLI34Kbn-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
