{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from tqdm.notebook import tqdm\n",
    "COLAB = False\n",
    "if not COLAB:\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Input\n",
    "from utils import ReplayBuffer, OrnsteinUhlenbeckActionNoise\n",
    "path_base = \"models/\"\n",
    "RESUME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "MAX_Money = 100000\n",
    "class StockEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self,dfs, train, number=1, **kwargs):\n",
    "        super(StockEnv,self).__init__()\n",
    "        self.train = train\n",
    "        self.MAX_shares = 2147483647\n",
    "        self.Min_Brokerage = 30\n",
    "        self.count = number\n",
    "        self.Brokerage_rate = 0.001\n",
    "        \n",
    "        if \"balance\" in kwargs.keys():\n",
    "            Max_Money = kwargs[\"balance\"]\n",
    "        if \"Max_Shares\" in kwargs.keys():\n",
    "            self.MAX_shares = kwargs[\"Shares\"]\n",
    "        if \"Broke_limit\" in kwargs.keys():\n",
    "            self.Min_Brokerage = kwargs[\"Broke_limit\"]\n",
    "        if \"Broke_rate\" in kwargs.keys():\n",
    "            self.Brokerage_rate = kwargs[\"Broke_rate\"]\n",
    "        \n",
    "        self.dfs = dfs\n",
    "        self.action_space = spaces.Box(low = np.array([-1]), high = np.array([1]), dtype = np.float16)\n",
    "        lower = [0]*number\n",
    "        higher = [1]*number\n",
    "        self.observation_space = spaces.Box(low=np.array(lower),high=np.array(higher),dtype=np.float32)\n",
    "    \n",
    "    def _get_prices(self):\n",
    "#         print (\"Day {0}\".format(self.df.loc[self.current_step,\"Date\"]))\n",
    "#         print (\"low: {0} high: {1}\".format(self.df.loc[self.current_step,\"Open\"],self.df.loc[self.current_step,\"Close\"]))\n",
    "        return np.array([np.random.uniform(df.loc[self.current_step,\"Open\"], df.loc[self.current_step,\"Close\"]) for df in self.dfs])\n",
    "    \n",
    "    def _observe(self, prices):\n",
    "        frame = prices\n",
    "        frame = frame / self.highest_price\n",
    "        info = {\n",
    "            'balance' : self.balance,\n",
    "            'highest_price': self.highest_price,\n",
    "            'current_price': self.current_prices,\n",
    "            #'time': self.df.loc[self.current_step,'time_stamp'],\n",
    "            'shares_held': self.shares_held,\n",
    "            'max_worth': self.max_net_worth,\n",
    "            'broke_limit': self.Min_Brokerage,\n",
    "            'broke_rate': self.Brokerage_rate\n",
    "        }\n",
    "        \n",
    "        return frame, info\n",
    "        \n",
    "    def reset(self,balance = MAX_Money,**kwargs):\n",
    "        if \"balance\" in kwargs.keys():\n",
    "            Max_Money = kwargs[\"balance\"]\n",
    "        if \"Max_Shares\" in kwargs.keys():\n",
    "            self.MAX_shares = kwargs[\"Shares\"]\n",
    "        if \"Broke_limit\" in kwargs.keys():\n",
    "            self.Min_Brokerage = kwargs[\"Broke_limit\"]\n",
    "        if \"Broke_rate\" in kwargs.keys():\n",
    "            self.Brokerage_rate = kwargs[\"Broke_rate\"]\n",
    "        \n",
    "        if self.train:\n",
    "            self.current_step = np.random.randint(0,len(self.dfs[0].loc[:,'Open'].values)-1)\n",
    "        else:\n",
    "            self.current_step = 0\n",
    "       \n",
    "        self.balance = balance\n",
    "        self.shares_held = np.array([0]*self.count)\n",
    "        self.current_prices = self._get_prices() \n",
    "        self.net_worth = self.balance + sum(self.shares_held*self.current_prices)\n",
    "        self.initial_worth = self.net_worth\n",
    "        self.max_net_worth = self.net_worth\n",
    "        self.highest_price = np.max(self.current_prices)\n",
    "        frame,_ =  self._observe(self.current_prices)\n",
    "        return frame\n",
    "    \n",
    "    def _broke(self,amount):\n",
    "        return max(amount * self.Brokerage_rate,self.Min_Brokerage)\n",
    "    \n",
    "    def _take_action(self, action_vector):\n",
    "        self.current_prices = self._get_prices()\n",
    "        self.highest_price = max(self.highest_price,np.max(self.current_prices))\n",
    "        action_vector  = action_vector*self.MAX_shares\n",
    "        for i in range(self.count):\n",
    "            if action_vector[i] < 0:\n",
    "                # sell\n",
    "                action_vector[i] = -1*action_vector[i]\n",
    "                if action_vector[i] > self.shares_held[i]:\n",
    "                    action_vector[i] = self.shares_held[i]\n",
    "                amount_gained = action_vector[i]*self.current_prices[i]\n",
    "                broke = self._broke(amount_gained)\n",
    "                amount_gained -= broke\n",
    "                if self.balance + amount_gained < 0:\n",
    "                    a1 = np.floor(self.balance/((self.Brokerage_rate-1)*self.current_prices[i]))\n",
    "                    action = np.floor(-(self.balance-self.Min_Brokerage)/self.current_prices[i])\n",
    "                    if self._broke(a1*self.current_prices[i]) == a1*self.current_prices[i]*self.Brokerage_rate:\n",
    "                        action_vector[i] = max(a1,action_vector[i])\n",
    "                    action_vector[i] = max(action_vector[i],0)\n",
    "                    amount_gained = action_vector[i]*self.current_price\n",
    "                    amount_gained -= self._broke(amount_gained)\n",
    "                self.balance +=amount_gained\n",
    "                self.shares_held[i] = self.shares_held[i]-action_vector[i]\n",
    "            elif action_vector[i]>0:\n",
    "                #buy\n",
    "                amount_required = self.current_prices[i]*action_vector[i] + self._broke(self.current_prices[i]*action_vector[i])\n",
    "                if amount_required > self.balance:\n",
    "                    a1 = np.floor(self.balance/((self.Brokerage_rate+1)*self.current_prices[i]))\n",
    "                    action_vector[i] = np.floor((self.balance-self.Min_Brokerage)/self.current_prices[i])\n",
    "                    if self._broke(a1*self.current_prices[i]) == a1*self.current_prices[i]*self.Brokerage_rate:\n",
    "                        action_vector[i] = max(a1,action_vector[i])\n",
    "                    action_vector[i] = max(action_vector[i],0)\n",
    "                    amount_required = action_vector[i]*self.current_prices[i]\n",
    "                    amount_required -= self._broke(amount_required)\n",
    "                self.balance -= amount_required\n",
    "                self.shares_held[i] += action_vector[i]\n",
    "        reward = self.balance + sum(self.shares_held* self.current_prices) - self.net_worth\n",
    "        self.net_worth = self.balance + sum(self.shares_held* self.current_prices)\n",
    "        if self.net_worth > self.max_net_worth:\n",
    "            self.max_net_worth = self.net_worth\n",
    "        return reward, self.current_prices\n",
    "            \n",
    "    def step(self, action):\n",
    "        reward, prices = self._take_action(action)\n",
    "        self.current_step+=1\n",
    "        if self.current_step > len(self.dfs[0].loc[:,'Open'].values)-1:\n",
    "            self.current_step = 0\n",
    "        \n",
    "        done = self.net_worth<=0\n",
    "        obs, info = self._observe(prices)\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def render(self, mode='human', close = False):\n",
    "        profit = self.net_worth - self.initial_worth\n",
    "        print(f'Step: {self.current_step}')\n",
    "        print(f'Net Worth:{self.net_worth}')\n",
    "        print(f'Profit: {profit}')\n",
    "\n",
    "\n",
    "def create_stock_env(locations, train=True):\n",
    "    dfs = [pd.read_csv(location) for location in locations]\n",
    "    for df in dfs:\n",
    "        (df.sort_values(\"Date\"))\n",
    "    return StockEnv(dfs, train, len(locations)), dfs[0].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor:\n",
    "    def __init__(self, params):\n",
    "        self.output_range = params[\"output_range\"]\n",
    "        self.hidden_layers = params[\"actor_hidden_layers\"]\n",
    "        self.state_dimensions = params[\"state_dimensions\"]\n",
    "        self.action_dimensions = params[\"action_dimensions\"]\n",
    "        self.actor = self.model()\n",
    "        \n",
    "    def model(self):\n",
    "        inputs = Input(shape=(1, self.state_dimensions))\n",
    "        x = Lambda(lambda x: x)(inputs)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = Dense(layer, activation='relu')(x)\n",
    "        x = Dense(self.action_dimensions, activation='tanh')(x)\n",
    "        x = Lambda(lambda x: x*self.output_range)(x)\n",
    "        model = tf.keras.Model(inputs = inputs, outputs = x)\n",
    "        return model\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state_tensor = tf.Variable(shape = state.shape, initial_value = state)\n",
    "        return (self.actor(state_tensor)).numpy()\n",
    "\n",
    "    def save_weights(self):\n",
    "        self.actor.save_weights(path_base + \"actor.h5\")\n",
    "               \n",
    "    def load_weights(self):\n",
    "        self.actor.load_weights(path_base + \"actor.h5\")\n",
    "        \n",
    "    \n",
    "class Critic:\n",
    "    def __init__(self, params):\n",
    "        self.hidden_layers = params[\"critic_hidden_layers\"]\n",
    "        self.state_dimensions = params[\"state_dimensions\"]\n",
    "        self.action_dimensions = params[\"action_dimensions\"]\n",
    "        self.optimizer = params[\"critic_optimizer\"]\n",
    "        self.critic_online = self.model()\n",
    "        self.critic_target = self.model()\n",
    "\n",
    "\n",
    "    def model(self):\n",
    "        input_a = Input(shape = (1, self.state_dimensions))\n",
    "        input_b = Input(shape = (1, self.action_dimensions))\n",
    "        x = concatenate([input_a, input_b], axis=-1)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = Dense(layer, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = tf.keras.Model(inputs=[input_a, input_b], outputs = x)\n",
    "        model.compile(loss='mse', optimizer=self.optimizer)\n",
    "        return model\n",
    "    \n",
    "    def save_weights(self):\n",
    "        self.critic_online.save_weights(path_base + \"critic_online.h5\")\n",
    "        self.critic_target.save_weights(path_base +  \"critic_target.h5\")\n",
    "               \n",
    "    def load_weights(self):\n",
    "        self.critic_online.load_weights(path_base + \"critic_online.h5\")\n",
    "        self.critic_target.load_weights(path_base + \"critic_target.h5\")\n",
    "\n",
    "    def get_qvalues(self, state_array, action_array, online=True):\n",
    "        state_tensor = tf.Variable(shape = state_array.shape, initial_value = state_array)\n",
    "        action_tensor = tf.Variable(shape = action_array.shape, initial_value = action_array)\n",
    "        return (self.critic_online([state_tensor, action_tensor]).numpy() if online else self.critic_target([state_tensor, action_tensor]).numpy())\n",
    "    \n",
    "    def call(self, state_tensor, action_tensor, online = True):\n",
    "        return (self.critic_online([state_tensor, action_tensor]) if online else self.critic_target([state_tensor, action_tensor]))\n",
    "    def merge_networks(self, tau):\n",
    "        self.critic_target.set_weights(tau*np.array(self.critic_online.get_weights())\n",
    "                                                                    + (1-tau)*np.array(self.critic_target.get_weights()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params, test=False):\n",
    "        self.test = test\n",
    "        self.actor = Actor(params)\n",
    "        self.critic = Critic(params)\n",
    "        self.buffer = ReplayBuffer(params[\"buffer_size\"])\n",
    "        self.state_dimensions = params[\"state_dimensions\"]\n",
    "        self.action_dimensions = params[\"action_dimensions\"]\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.action_range = params[\"output_range\"]\n",
    "        self.save_frequency = params[\"save_frequency\"]\n",
    "        self.batch_size = params[\"batch_size\"]\n",
    "        self.optimizer = params[\"actor_optimizer\"]\n",
    "        self.tau = params[\"tau\"]\n",
    "        self.step = 0\n",
    "        self.noise_func =  OrnsteinUhlenbeckActionNoise(mu=np.zeros(params[\"action_dimensions\"]))\n",
    "        if RESUME:\n",
    "            self.load_networks()\n",
    "        \n",
    "    def agent_start(self, observation):\n",
    "        observation = np.reshape(observation, (1, self.state_dimensions))\n",
    "        act = np.squeeze(self.actor.get_action(observation))\n",
    "        if not self.test:\n",
    "            for i in range(act.shape[0]):\n",
    "                act[i] = np.squeeze(self.clip_action((act[i] + self.noise_func())[0]))\n",
    "        else:\n",
    "            for i in range(act.shape[0]):\n",
    "                act[i] = np.squeeze(self.clip_action(act[i]))\n",
    "        self.prev_state = observation\n",
    "        self.prev_action = act\n",
    "        return act\n",
    "\n",
    "    def clip_action(self, action):\n",
    "        if abs(action) > self.action_range:\n",
    "            action *= abs(self.action_range)/abs(action)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_step(self, reward, observation):\n",
    "        observation = np.reshape(observation, (1, self.state_dimensions))\n",
    "        if not self.test:\n",
    "            relay = (self.prev_state, self.prev_action, reward, observation)\n",
    "            self.buffer.add(relay)\n",
    "        self.prev_state = observation\n",
    "        act = np.squeeze(self.actor.get_action(observation))\n",
    "        if not self.test:\n",
    "            self.prev_action = self.clip_action(act + self.noise_func())\n",
    "            self.train(self.batch_size)\n",
    "        else:\n",
    "            self.prev_action = [self.clip_action(act)]\n",
    "        \n",
    "        return self.prev_action \n",
    "    \n",
    "    def save_networks(self):\n",
    "        self.actor.save_weights()\n",
    "        self.critic.save_weights()\n",
    "\n",
    "    def load_networks(self):\n",
    "        self.actor.load_weights()\n",
    "        self.critic.load_weights()\n",
    "\n",
    "\n",
    "    def train(self, sample_size):\n",
    "        self.step += 1\n",
    "        batch, batch_size = self.buffer.sample(sample_size)\n",
    "\n",
    "        state_array = np.array([ element[3] for element in batch])\n",
    "        action_array = self.actor.get_action(state_array)\n",
    "        prev_state_array = np.array([ element[0] for element in batch])\n",
    "        prev_action_array = np.array([ [[element[2]]] for element in batch])\n",
    "        output = self.critic.get_qvalues(state_array, action_array, False)\n",
    "        output = np.array([element[2] + self.discount*out[0] for element, out in zip(batch, output)])\n",
    "        self.critic.critic_online.fit([state_array, action_array], output, verbose=0)\n",
    "\n",
    "        prev_state_tensor = tf.Variable(shape = prev_state_array.shape, initial_value = prev_state_array)\n",
    "        prev_action_tensor = tf.Variable(shape = prev_action_array.shape, initial_value = prev_action_array)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as g:\n",
    "            g.watch(prev_action_tensor) \n",
    "            g.watch(prev_state_tensor)\n",
    "            value = self.critic.call(prev_state_tensor, prev_action_tensor)\n",
    "            action = self.actor.actor(prev_state_tensor)\n",
    "            \n",
    "        gradient = -tf.squeeze(g.gradient(value, prev_action_tensor))\n",
    "        gradient = tf.cast(gradient, tf.float32)\n",
    "        gradient_actor = g.gradient(action, self.actor.actor.trainable_weights, gradient)\n",
    "        gradient_actor = list(map(lambda x: tf.math.divide(x, batch_size), gradient_actor))\n",
    "        self.optimizer.apply_gradients(zip(gradient_actor, self.actor.actor.trainable_weights))\n",
    "        self.critic.merge_networks(self.tau)\n",
    "\n",
    "        if self.step%self.save_frequency == 0:\n",
    "            self.save_networks()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_PARAMS = {\n",
    "\t\"output_range\": 1,\n",
    "\t\"actor_hidden_layers\": [60, 16],\n",
    "\t\"critic_hidden_layers\": [60, 16],\n",
    "\t\"state_dimensions\": 29,\n",
    "\t\"action_dimensions\": 29,\n",
    "\t\"critic_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "\t\"actor_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "\t\"batch_size\": 64,\n",
    "\t\"buffer_size\":1000000,\n",
    "\t\"discount\": 0.99,\n",
    "\t\"tau\": 0.001,\n",
    "\t\"save_frequency\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [\"train/\" + f for f in os.listdir(\"train/\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, stamps = create_stock_env(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34131135, 0.42253686, 0.23074003, 0.49295835, 0.23768464,\n",
       "       0.49309801, 0.50289128, 0.45724847, 0.97002253, 0.31276553,\n",
       "       0.72543937, 0.45306388, 0.23746844, 0.48913126, 0.15434905,\n",
       "       0.66211881, 0.5361211 , 0.17378914, 0.59805951, 0.2758236 ,\n",
       "       0.30583717, 0.44894854, 0.53689521, 1.        , 0.12496449,\n",
       "       0.32904595, 0.50755846, 0.4530461 , 0.26231207])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.34053008, 0.42214879, 0.23102212, 0.49124809, 0.23773338,\n",
       "        0.493377  , 0.50198586, 0.45811289, 0.97118697, 0.30899051,\n",
       "        0.72549735, 0.45374968, 0.2383658 , 0.48890162, 0.15507515,\n",
       "        0.66051056, 0.52666711, 0.17403264, 0.60927722, 0.27638189,\n",
       "        0.3060475 , 0.44998448, 0.54177366, 0.99905866, 0.12496449,\n",
       "        0.32891922, 0.50763107, 0.4491711 , 0.26166008]),\n",
       " 939.9137519145443,\n",
       " False,\n",
       " {'balance': 71.25027630259115,\n",
       "  'highest_price': 184.5324148206363,\n",
       "  'current_price': array([ 62.83883768,  77.90013554,  42.63106939,  90.65119689,\n",
       "          43.86951473,  91.04404938,  92.63266222,  84.53667734,\n",
       "         179.21547706,  57.01876513, 133.87777871,  83.73152481,\n",
       "          43.98621644,  90.21819709,  28.61639171, 121.88560903,\n",
       "          97.18715337,  32.11466331, 112.43139676,  51.00141849,\n",
       "          56.47568476,  83.03672233,  99.97480106, 184.35870616,\n",
       "          23.059999  ,  60.69625728,  93.6743867 ,  82.88662841,\n",
       "          48.28476587]),\n",
       "  'shares_held': array([1590,    2,    0,    0,    2,    0,    0,    1,    0,    1,    0,\n",
       "            0,    2,    0,    2,    0,    0,    3,    0,    1,    0,    0,\n",
       "            1,    0,    2,    0,    0,    1,    1]),\n",
       "  'max_worth': 100939.91375191454,\n",
       "  'broke_limit': 30,\n",
       "  'broke_rate': 0.001})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e34051c74a54a2ab89868a2ec6af335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration: ', max=20000, style=ProgressStyle(description_widtâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'StockEnv' object has no attribute 'current_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-935492a38bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Iteration: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprofit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprev_profit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-98b2e3872ffe>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Open'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-98b2e3872ffe>\u001b[0m in \u001b[0;36m_take_action\u001b[0;34m(self, action_vector)\u001b[0m\n\u001b[1;32m    101\u001b[0m                         \u001b[0maction_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0maction_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mamount_gained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_price\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mamount_gained\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_broke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamount_gained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0mamount_gained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StockEnv' object has no attribute 'current_price'"
     ]
    }
   ],
   "source": [
    "agent = Agent(AGENT_PARAMS)\n",
    "ITERATIONS = 20000\n",
    "pbar = tqdm(desc=\"Iteration: \", total=ITERATIONS)\n",
    "action = agent.agent_start(env.reset())\n",
    "observation, reward, done, info = env.step(action)\n",
    "profit = np.zeros(ITERATIONS)\n",
    "prev_profit = 0\n",
    "for _ in range(ITERATIONS):\n",
    "    action = agent.agent_step(reward, observation)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    profit[i] += reward + prev_profit\n",
    "    prev_profit += reward\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
