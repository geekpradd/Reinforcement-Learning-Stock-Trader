{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning to play Atari from RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.epsilon = params[\"epsilon\"]\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.frame_size = params[\"frame_size\"]\n",
    "        self.num_actions = params[\"actions\"]\n",
    "        self.optimizer = params[\"optimizer\"]\n",
    "        self.memory = deque()\n",
    "        self.experience = list()\n",
    "        self.current_index = 0\n",
    "        self.q_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        default = np.zeros((128, ))\n",
    "        for _ in range(5):\n",
    "            self.memory.append(default)\n",
    "        \n",
    "    def merge_networks(self, tau):\n",
    "        self.target_network.set_weights(tau*np.array(self.q_network.get_weights())\n",
    "                                        + (1-tau)*np.array(self.target_network.get_weights()))\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(80, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer= self.optimizer)\n",
    "        return model\n",
    "        \n",
    "    def get_input(self, state):\n",
    "        self.memory.append(np.array(state))\n",
    "        if len(self.memory) > self.frame_size:\n",
    "            self.memory.popleft()\n",
    "        input_layer = np.array([])\n",
    "        for frame in self.memory:\n",
    "            input_layer = np.concatenate([input_layer, frame])\n",
    "        \n",
    "        return np.array([input_layer])\n",
    "        \n",
    "    def agent_start(self, start_state):\n",
    "        input_layer = self.get_input(start_state)\n",
    "        print (input_layer.shape)\n",
    "        q_values = self.q_network.predict(input_layer)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        input_layer = self.get_input(state)\n",
    "        q_values = self.q_network.predict(input_layer)\n",
    "        relay = (self.prev_state, self.prev_action,  reward, input_layer, 0)\n",
    "        self.experience.append(relay)\n",
    "        self.train(5)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "        \n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        relay = (self.prev_state, self.prev_action,  reward, 0, 1)\n",
    "        self.experience.append(relay)\n",
    "        self.train(5)\n",
    "        self.experience.clear()\n",
    "        \n",
    "        \n",
    "    def train(self, count):\n",
    "        batch = random.sample(self.experience, min(count, len(self.experience)))\n",
    "        for state, action, reward, future, terminated in batch:\n",
    "            target = self.q_network.predict(state)\n",
    "            updated = reward\n",
    "            if not terminated:\n",
    "                target_vals = self.target_network.predict(future)[0]\n",
    "                updated += self.discount*np.amax(target_vals)\n",
    "                \n",
    "            target[0][action] = updated\n",
    "            \n",
    "            self.q_network.fit(state, target, epochs=1, verbose=0)\n",
    "            \n",
    "        self.merge_networks(0.01)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "params = {\"epsilon\":0.1, \"discount\": 1, \"frame_size\": 3, \"actions\": 6, \"optimizer\": optimizer}\n",
    "agent = Agent(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0\n",
      "(1, 640)\n",
      "Done False at Count 0\n",
      "Done False at Count 1\n",
      "Done False at Count 2\n",
      "Done False at Count 3\n",
      "Done False at Count 4\n",
      "Done False at Count 5\n",
      "Done False at Count 6\n",
      "Done False at Count 7\n",
      "Done False at Count 8\n",
      "Done False at Count 9\n",
      "Done False at Count 10\n",
      "Done False at Count 11\n",
      "Done False at Count 12\n",
      "Done False at Count 13\n",
      "Done False at Count 14\n",
      "Done False at Count 15\n",
      "Done False at Count 16\n",
      "Done False at Count 17\n",
      "Done False at Count 18\n",
      "Done False at Count 19\n",
      "Done False at Count 20\n",
      "Done False at Count 21\n",
      "Done False at Count 22\n",
      "Done False at Count 23\n",
      "Done False at Count 24\n",
      "Done False at Count 25\n",
      "Done False at Count 26\n",
      "Done False at Count 27\n",
      "Done False at Count 28\n",
      "Done False at Count 29\n",
      "Done False at Count 30\n",
      "Done False at Count 31\n",
      "Done False at Count 32\n",
      "Done False at Count 33\n",
      "Done False at Count 34\n",
      "Done False at Count 35\n",
      "Done False at Count 36\n",
      "Done False at Count 37\n",
      "Done False at Count 38\n",
      "Done False at Count 39\n",
      "Done False at Count 40\n",
      "Done False at Count 41\n",
      "Done False at Count 42\n",
      "Done False at Count 43\n",
      "Done False at Count 44\n",
      "Done False at Count 45\n",
      "Done False at Count 46\n",
      "Done False at Count 47\n",
      "Done False at Count 48\n",
      "Done False at Count 49\n",
      "Done False at Count 50\n",
      "Done False at Count 51\n",
      "Done False at Count 52\n",
      "Done False at Count 53\n",
      "Done False at Count 54\n",
      "Done False at Count 55\n",
      "Done False at Count 56\n",
      "Done False at Count 57\n",
      "Done False at Count 58\n",
      "Done False at Count 59\n",
      "Done False at Count 60\n",
      "Done False at Count 61\n",
      "Done False at Count 62\n",
      "Done False at Count 63\n",
      "Done False at Count 64\n",
      "Done False at Count 65\n",
      "Done False at Count 66\n",
      "Done False at Count 67\n",
      "Done False at Count 68\n",
      "Done False at Count 69\n",
      "Done False at Count 70\n",
      "Done False at Count 71\n",
      "Done False at Count 72\n",
      "Done False at Count 73\n",
      "Done False at Count 74\n",
      "Done False at Count 75\n",
      "Done False at Count 76\n",
      "Done False at Count 77\n",
      "Done False at Count 78\n",
      "Done False at Count 79\n",
      "Done False at Count 80\n",
      "Done False at Count 81\n",
      "Done False at Count 82\n",
      "Done False at Count 83\n",
      "Done False at Count 84\n",
      "Done False at Count 85\n",
      "Done False at Count 86\n",
      "Done False at Count 87\n",
      "Done False at Count 88\n",
      "Done False at Count 89\n",
      "Done False at Count 90\n",
      "Done False at Count 91\n",
      "Done False at Count 92\n",
      "Done False at Count 93\n",
      "Done False at Count 94\n",
      "Done False at Count 95\n",
      "Done False at Count 96\n",
      "Done False at Count 97\n",
      "Done False at Count 98\n",
      "Done False at Count 99\n",
      "Done False at Count 100\n",
      "Done False at Count 101\n",
      "Done False at Count 102\n",
      "Done False at Count 103\n",
      "Done False at Count 104\n",
      "Done False at Count 105\n",
      "Done False at Count 106\n",
      "Done False at Count 107\n",
      "Done False at Count 108\n",
      "Done False at Count 109\n",
      "Done False at Count 110\n",
      "Done False at Count 111\n",
      "Done False at Count 112\n",
      "Done False at Count 113\n",
      "Done False at Count 114\n",
      "Done False at Count 115\n",
      "Done False at Count 116\n",
      "Done False at Count 117\n",
      "Done False at Count 118\n",
      "Done False at Count 119\n",
      "Done False at Count 120\n",
      "Done False at Count 121\n",
      "Done False at Count 122\n",
      "Done False at Count 123\n",
      "Done False at Count 124\n",
      "Done False at Count 125\n",
      "Done False at Count 126\n",
      "Done False at Count 127\n",
      "Done False at Count 128\n",
      "Done False at Count 129\n",
      "Done False at Count 130\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 100\n",
    "for _ in range(ITERATIONS):\n",
    "    print (\"Game {0}\".format(_))\n",
    "    action = agent.agent_start(env.reset())\n",
    "    observation, reward, done, info = env.step([action])\n",
    "    count = 0\n",
    "    while not done:\n",
    "        action = agent.agent_step(reward, observation)\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        print (\"Done {0} at Count {1}\".format(done, count))\n",
    "        count += 1\n",
    "    agent.agent_end(reward)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            multiple                  51280     \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            multiple                  486       \n",
      "=================================================================\n",
      "Total params: 51,766\n",
      "Trainable params: 51,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
