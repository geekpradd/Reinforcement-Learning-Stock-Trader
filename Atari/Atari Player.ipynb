{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning to play Atari from RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import gym, random\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "RESUME = False\n",
    "path_base = \"models/\"\n",
    "if COLAB:\n",
    "    path_base = \"drive/My Drive/\"\n",
    "env = gym.make('Pong-ram-v0')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.epsilon = 1\n",
    "        self.decrease_till = params[\"annealation_count\"]\n",
    "        self.rate_per_step = 0.9/self.decrease_till\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.merge_frequency = params[\"merge_frequency\"]\n",
    "        self.save_frequency = params[\"save_frequency\"]\n",
    "        self.frame_size = params[\"frame_size\"]\n",
    "        self.num_actions = params[\"actions\"]\n",
    "        self.optimizer = params[\"optimizer\"]\n",
    "        self.experience_memory = params[\"memory\"]\n",
    "        self.memory = deque()\n",
    "        self.experience = list()\n",
    "        self.q_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.count = 0\n",
    "        self.game = 0\n",
    "        default = np.zeros((128, ))\n",
    "        for _ in range(self.frame_size):\n",
    "            self.memory.append(default)\n",
    "        \n",
    "    def merge_networks(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        model.add(tf.keras.layers.Dense(100, activation='relu', \n",
    "                        input_shape=(128*self.frame_size, ), kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, activation='linear', kernel_initializer=initializer))\n",
    "        model.compile(loss='mse', optimizer= self.optimizer)\n",
    "        return model\n",
    "        \n",
    "    def get_input(self, state):\n",
    "        state = state/255\n",
    "        self.memory.append(np.array(state))\n",
    "        if len(self.memory) > self.frame_size:\n",
    "            self.memory.popleft()\n",
    "        input_layer = np.array([])\n",
    "        for frame in self.memory:\n",
    "            input_layer = np.concatenate([input_layer, frame])\n",
    "        \n",
    "        return input_layer\n",
    "        \n",
    "    def agent_start(self, start_state):\n",
    "        input_layer = self.get_input(start_state)\n",
    "        q_values = self.q_network.predict(np.array([input_layer]))\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values[0])\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        self.count += 1\n",
    "        if self.count > self.decrease_till:\n",
    "            self.epsilon = 0.1\n",
    "        else:\n",
    "            self.epsilon = 1 - self.count*self.rate_per_step\n",
    "        input_layer = self.get_input(state)\n",
    "        q_values = self.q_network.predict(np.array([input_layer]))\n",
    "        relay = (self.prev_state, self.prev_action,  reward, input_layer, 0)\n",
    "        self.experience.append(relay)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values[0])\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action, q_values[0][action]\n",
    "        \n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        relay = (self.prev_state, self.prev_action,  reward, self.prev_state, 1)\n",
    "        self.experience.append(relay)\n",
    "        self.train(10000)\n",
    "        self.game += 1\n",
    "        if len(self.experience) > self.experience_memory:\n",
    "            self.experience.clear()\n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.q_network.save_weights(path_base + \"q.h5\")\n",
    "        self.target_network.save_weights(path_base + \"target.h5\")\n",
    "#         with open(path_base + \"agent.obj\", \"wb\") as f:\n",
    "#             pickle.dump(self, f)\n",
    "        \n",
    "    def train(self, count):\n",
    "        batch = random.sample(self.experience, min(count, len(self.experience)))\n",
    "        step = 0\n",
    "        input_tensor = [state for state, action, reward, future, terminated in batch]\n",
    "        output_tensor = self.q_network.predict(np.array(input_tensor))\n",
    "        future_input_tensor = [future for state, action, reward, future, terminated in batch]\n",
    "        future_out = self.target_network.predict(np.array(future_input_tensor))\n",
    "        for count, (state, action, reward, future, terminated) in enumerate(batch):\n",
    "            target = output_tensor[count]\n",
    "            updated = reward\n",
    "            if not terminated:\n",
    "                target_vals = future_out[count]\n",
    "                updated += self.discount*np.amax(target_vals)\n",
    "                \n",
    "            target[action] = updated\n",
    "            output_tensor[count] = target \n",
    "        \n",
    "        input_tensor = np.array(input_tensor)\n",
    "        output_tensor = np.array(output_tensor)\n",
    "        self.q_network.fit(input_tensor, output_tensor, epochs=1, verbose=0)\n",
    "            \n",
    "        \n",
    "        if self.game%self.merge_frequency == 0:\n",
    "            self.merge_networks()\n",
    "            \n",
    "        if self.game%self.save_frequency == 0:\n",
    "            self.save_weights()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.05)\n",
    "params = {\"annealation_count\":100000, \"merge_frequency\": 7, \"save_frequency\": 10, \"discount\": 1, \"frame_size\": 3, \"actions\": 6, \"optimizer\": optimizer, \"memory\": 10000}\n",
    "agent = Agent(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca13d636cf24582aad7e8fe8f4d3ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Game: ', max=400, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edd606206ac4be1ab245034fd49e919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Game Step: ', max=1, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ITERATIONS = 400\n",
    "np.set_printoptions(precision=3)\n",
    "render = True\n",
    "gbar = tqdm(desc=\"Game: \", total=ITERATIONS)\n",
    "pbar = tqdm(desc=\"Game Step: \")\n",
    "y = []\n",
    "x = []\n",
    "for _ in range(ITERATIONS):\n",
    "    action = agent.agent_start(env.reset())\n",
    "    observation, reward, done, info = env.step([action])\n",
    "    \n",
    "    game_reward = 0\n",
    "    while not done:\n",
    "        action, value = agent.agent_step(reward, observation)\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        game_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        pbar.set_description(\"Action Value {0} Game Step: \".format(value))\n",
    "        pbar.update(1)\n",
    "        \n",
    "    y.append(game_reward)\n",
    "    x.append(_)\n",
    "    pl.plot(x, y)\n",
    "    gbar.update(1)\n",
    "    pbar.refresh()\n",
    "    pbar.reset()\n",
    "    agent.agent_end(reward)\n",
    "    pl.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 31,286\n",
      "Trainable params: 31,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
