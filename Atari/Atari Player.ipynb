{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning to play Atari from RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8dc90e9f4c06>:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Pong-ram-v0')\n",
    "tf.test.is_gpu_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.epsilon = params[\"epsilon\"]\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.frame_size = params[\"frame_size\"]\n",
    "        self.num_actions = params[\"actions\"]\n",
    "        self.optimizer = params[\"optimizer\"]\n",
    "        self.memory = deque()\n",
    "        self.experience = list()\n",
    "        self.current_index = 0\n",
    "        self.q_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        default = np.zeros((128, ))\n",
    "        for _ in range(5):\n",
    "            self.memory.append(default)\n",
    "        \n",
    "    def merge_networks(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(80, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer= self.optimizer)\n",
    "        return model\n",
    "        \n",
    "    def get_input(self, state):\n",
    "        self.memory.append(np.array(state))\n",
    "        if len(self.memory) > self.frame_size:\n",
    "            self.memory.popleft()\n",
    "        input_layer = np.array([])\n",
    "        for frame in self.memory:\n",
    "            input_layer = np.concatenate([input_layer, frame])\n",
    "        \n",
    "        return np.array([input_layer])\n",
    "        \n",
    "    def agent_start(self, start_state):\n",
    "        input_layer = self.get_input(start_state)\n",
    "        print (input_layer.shape)\n",
    "        q_values = self.q_network.predict(input_layer)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, state):\n",
    "        input_layer = self.get_input(state)\n",
    "        q_values = self.q_network.predict(input_layer)\n",
    "        relay = (self.prev_state, self.prev_action,  reward, input_layer, 0)\n",
    "        self.experience.append(relay)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = input_layer\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "        \n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        relay = (self.prev_state, self.prev_action,  reward, 0, 1)\n",
    "        self.experience.append(relay)\n",
    "        self.train(1000)\n",
    "        if len(self.experience) > 40000:\n",
    "            self.experience.clear()\n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.q_network.save_weights(\"q.h5\")\n",
    "        self.target_network.save_weigts(\"target.h5\")\n",
    "        \n",
    "    def train(self, count):\n",
    "        batch = random.sample(self.experience, min(count, len(self.experience)))\n",
    "        step = 0\n",
    "        for state, action, reward, future, terminated in batch:\n",
    "            step += 1\n",
    "            if step%100 == 1:\n",
    "                print (\"Step {0} for training step\".format(step))\n",
    "            target = self.q_network.predict(state)\n",
    "            updated = reward\n",
    "            if not terminated:\n",
    "                target_vals = self.target_network.predict(future)[0]\n",
    "                updated += self.discount*np.amax(target_vals)\n",
    "                \n",
    "            target[0][action] = updated\n",
    "            \n",
    "            self.q_network.fit(state, target, epochs=1, verbose=0)\n",
    "            \n",
    "        self.merge_networks()\n",
    "        \n",
    "        self.save_weights()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n",
    "params = {\"epsilon\":0.1, \"discount\": 1, \"frame_size\": 3, \"actions\": 6, \"optimizer\": optimizer}\n",
    "agent = Agent(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0\n",
      "(1, 640)\n",
      "About to train for game 0\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 1\n",
      "(1, 640)\n",
      "About to train for game 1\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 2\n",
      "(1, 640)\n",
      "About to train for game 2\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 3\n",
      "(1, 640)\n",
      "About to train for game 3\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 4\n",
      "(1, 640)\n",
      "About to train for game 4\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 5\n",
      "(1, 640)\n",
      "About to train for game 5\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 6\n",
      "(1, 640)\n",
      "About to train for game 6\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 7\n",
      "(1, 640)\n",
      "About to train for game 7\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 8\n",
      "(1, 640)\n",
      "About to train for game 8\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 9\n",
      "(1, 640)\n",
      "About to train for game 9\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 10\n",
      "(1, 640)\n",
      "About to train for game 10\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n",
      "Step 801 for training step\n",
      "Step 901 for training step\n",
      "Game 11\n",
      "(1, 640)\n",
      "About to train for game 11\n",
      "Step 1 for training step\n",
      "Step 101 for training step\n",
      "Step 201 for training step\n",
      "Step 301 for training step\n",
      "Step 401 for training step\n",
      "Step 501 for training step\n",
      "Step 601 for training step\n",
      "Step 701 for training step\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 100\n",
    "for _ in range(ITERATIONS):\n",
    "    print (\"Game {0}\".format(_))\n",
    "    action = agent.agent_start(env.reset())\n",
    "    observation, reward, done, info = env.step([action])\n",
    "    count = 0\n",
    "    while not done:\n",
    "        action = agent.agent_step(reward, observation)\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        count += 1\n",
    "    print (\"About to train for game {0}\".format(_))\n",
    "    agent.agent_end(reward)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            multiple                  51280     \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            multiple                  486       \n",
      "=================================================================\n",
      "Total params: 51,766\n",
      "Trainable params: 51,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
