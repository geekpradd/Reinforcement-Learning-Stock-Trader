{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "New_Stock.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iN7uufCUjIRO-PvNn8Zvql2GPbTQveRI",
      "authorship_tag": "ABX9TyPWt+6pBU1lmHM3upWkuHfc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86c6302fc52444878bfac1a14a816949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_249e0ffddb02442da42324b01dd24871",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_be21174636584de79bb117802cd8f11d",
              "IPY_MODEL_0e3a891c284e45058e731f32cfde6b58"
            ]
          }
        },
        "249e0ffddb02442da42324b01dd24871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be21174636584de79bb117802cd8f11d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_427a2c5bcde14c9ca2a3ac8fab5cba52",
            "_dom_classes": [],
            "description": " 48%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 958,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_24d9f541380544b3a3d5327d6763f3e7"
          }
        },
        "0e3a891c284e45058e731f32cfde6b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8be1411bf27b42d49ad8705001f714a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 958/2000 [00:54&lt;00:59, 17.59it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9fa16c33bfc2447a8d14a9059baf66eb"
          }
        },
        "427a2c5bcde14c9ca2a3ac8fab5cba52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "24d9f541380544b3a3d5327d6763f3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8be1411bf27b42d49ad8705001f714a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9fa16c33bfc2447a8d14a9059baf66eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geekpradd/Reinforcement-Learning-Stock-Trader/blob/master/New_Stock_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL-H71zCQ23m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "from gym import spaces\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm_notebook\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Concatenate, Lambda, Activation\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow import convert_to_tensor as convert\n",
        "import pickle\n",
        "# COLAB = False\n",
        "# if not COLAB:\n",
        "#     import os\n",
        "#     os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
        "path_base = '/content/drive/My Drive/Stock/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8618sorQ4t5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StockEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "    \n",
        "    def __init__(self, df, params, train = True):\n",
        "        super(StockEnv,self).__init__()\n",
        "        \n",
        "        self.num_stocks = params['num_stocks']\n",
        "        self.min_brokerage = params['min_brokerage']\n",
        "        self.brokerage_rate = params['brokerage_rate']\n",
        "        self.balance_normal = params['balance_normal']\n",
        "        self.shares_normal = params['shares_normal']\n",
        "        self.volume_normal = params['volume_normal']\n",
        "        self.dfs = df\n",
        "        self.state_dimensions = self.num_stocks*5+1\n",
        "        self.train = train\n",
        "\n",
        "        assert len(df) == self.num_stocks, \"Size of database not equal to number of stocks\"\n",
        "\n",
        "        self.max_steps = min([len(d.loc[:,'Open']) for d in self.dfs])\n",
        "        self.action_space = spaces.Box(low = -1, high = 1, shape =  (1, self.num_stocks*2), dtype = np.float32)\n",
        "        self.observation_space = spaces.Box(low = 0, high = 1, shape = (1, self.state_dimensions), dtype = np.float32)\n",
        "\n",
        "    def reset(self, intial_balance = 10000, shares_held = None):\n",
        "\n",
        "        if self.train:\n",
        "            self.current_step = np.random.randint(0, self.max_steps)\n",
        "        else:\n",
        "            self.current_step = 0\n",
        "        self.balance = intial_balance\n",
        "        self.shares_held = shares_held\n",
        "        if self.shares_held is None:\n",
        "            self.shares_held = np.zeros((1, self.num_stocks))\n",
        "        self.current_price = self.get_price()\n",
        "        self.highest_price = 0\n",
        "        self.net_worth = self.balance + np.sum(self.shares_held*self.current_price)\n",
        "        self.initial_worth = self.net_worth\n",
        "        self.max_net_worth = self.net_worth\n",
        "        self.set_high()\n",
        "        self.done = False\n",
        "        self.frame = np.zeros((1, self.state_dimensions))\n",
        "        self.info = {\n",
        "            'current_step' : self.current_step,\n",
        "            'current_price': self.current_price,\n",
        "            'highest_price': self.highest_price,\n",
        "            'net_worth' : self.net_worth,\n",
        "            'max_net_worth': self.max_net_worth,\n",
        "            'shares_held' : self.shares_held,\n",
        "            'shares_normal' : self.shares_normal,\n",
        "            'balance_normal' : self.balance_normal,\n",
        "            'balance' : self.balance,\n",
        "        }\n",
        "        return self.observe()\n",
        "        \n",
        "    def get_price(self):\n",
        "        return np.array([np.random.uniform(df.loc[self.current_step,\"Low\"], df.loc[self.current_step,\"High\"]) for df in self.dfs]).reshape((1, self.num_stocks))\n",
        "      \n",
        "    def set_high(self):\n",
        "        high = np.array([df.loc[self.current_step, 'High'] for df in self.dfs]).reshape((1, self.num_stocks))\n",
        "        self.highest_price = np.maximum(self.highest_price, high)\n",
        "\n",
        "    def observe(self):\n",
        "        for i in range(self.num_stocks):\n",
        "            self.frame[0, 4*i:4*i+4] = np.array([self.dfs[i].loc[self.current_step,'Open'],self.dfs[i].loc[self.current_step,'High'],self.dfs[i].loc[self.current_step,'Low'],self.dfs[i].loc[self.current_step,'Close']])/self.highest_price[0, i]\n",
        "        self.frame[0, self.num_stocks*4:self.num_stocks*5] = self.shares_held/self.shares_normal\n",
        "        self.frame[0, 5*self.num_stocks] = self.balance/self.balance_normal\n",
        "        self.info = {\n",
        "            'current_step' : self.current_step,\n",
        "            'current_price': self.current_price,\n",
        "            'highest_price': self.highest_price,\n",
        "            'net_worth' : self.net_worth,\n",
        "            'max_net_worth': self.max_net_worth,\n",
        "            'shares_held' : self.shares_held,\n",
        "            'shares_normal' : self.shares_normal,\n",
        "            'balance_normal' : self.balance_normal,\n",
        "            'balance' : self.balance\n",
        "        }\n",
        "        return self.frame, self.info\n",
        "    \n",
        "    def update_worth(self, reward):\n",
        "        self.net_worth += reward\n",
        "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
        "\n",
        "    def update_balance(self, action):\n",
        "        self.balance += np.sum(action[:, :self.num_stocks]*self.current_price)\n",
        "        self.balance -= np.sum(action[:, self.num_stocks:])\n",
        "\n",
        "    def update_shares(self, action):\n",
        "        self.shares_held -= action[:, :self.num_stocks]\n",
        "        buy = action[:, self.num_stocks:]/self.current_price\n",
        "        self.shares_held += buy\n",
        "\n",
        "    def take_action(self, action):\n",
        "        action[:, :self.num_stocks] *= self.shares_held\n",
        "        action[:, self.num_stocks:] *= self.balance\n",
        "        self.current_price = self.get_price()\n",
        "        self.set_high()\n",
        "        self.update_balance(action)\n",
        "        self.update_shares(action)\n",
        "        reward = self.balance + np.sum(self.shares_held * self.current_price) - self.net_worth\n",
        "        self.update_worth(reward)\n",
        "        return reward\n",
        "            \n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.max_steps or self.done:\n",
        "            self.done = True\n",
        "            return np.zeros((1, self.state_dimensions)), 0, self.done, self.info\n",
        "        if np.sum(action[:, self.num_stocks:]) > 1:\n",
        "            print('gadbad')\n",
        "        reward = self.take_action(action)\n",
        "        self.done = self.net_worth <= self.initial_worth*0.05\n",
        "        if self.done:\n",
        "            print('snap')\n",
        "        obs, info = self.observe()\n",
        "        return obs, reward, self.done, info\n",
        "    \n",
        "    def render(self, mode='human', close = False):\n",
        "        profit = self.net_worth - self.initial_worth\n",
        "        print('Step: {}'.format(self.current_step))\n",
        "        print('Net Worth: {}'.format(self.net_worth))\n",
        "        print('Profit: {}'.format(profit))\n",
        "        \n",
        "def create_stock_env(locations, train=True):\n",
        "    dfs = [pd.read_csv(location).sort_values('Date') for location in locations]\n",
        "    params = {\n",
        "        'num_stocks' : 2,\n",
        "        'min_brokerage' : 30.0,\n",
        "        'brokerage_rate' : 0.001,\n",
        "        'balance_normal' : 1000000,\n",
        "        'shares_normal' : 10000,\n",
        "        'volume_normal' : 2147483647,\n",
        "    }\n",
        "    return StockEnv(dfs, params, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Caoah1T3Q6UV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = [None] * max_size\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = random.sample(range(self.size), batch_size)\n",
        "        return [self.buffer[index] for index in indices]\n",
        "        \n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzsZozkwQ8Z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, params):\n",
        "        self.output_range = params[\"output_range\"]\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.cap = params['cap']\n",
        "        self.actor = self.build_model()\n",
        "        \n",
        "    def build_model(self):\n",
        "        inputs = Input(shape=(self.state_dimensions, ))\n",
        "        x = Dense(60, activation = 'relu')(inputs)\n",
        "        x = Dense(16, activation = 'relu')(x)\n",
        "        sell = Dense(self.action_dimensions, activation = 'sigmoid')(x)\n",
        "        buy = Dense(self.action_dimensions, activation = 'sigmoid')(x)\n",
        "        final_buy = Activation(tf.keras.activations.softmax)(buy)*tf.math.minimum(self.cap, tf.reduce_sum(buy, axis = -1, keepdims = True))\n",
        "        model = keras.Model(inputs = inputs, outputs = tf.concat([sell, final_buy], axis = -1))\n",
        "        # model.summary()\n",
        "        return model\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        return self.actor(convert(state))\n",
        "\n",
        "    def save(self):\n",
        "        self.actor.save(path_base + 'actor.h5')\n",
        "    \n",
        "    def load(self):\n",
        "        self.actor = keras.models.load_model(path_base + 'actor.h5')\n",
        "        print('Successfully Loaded')\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, params):\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.optimizer = params[\"critic_optimizer\"]\n",
        "        self.tau = params['tau']\n",
        "        self.critic_online = self.build_model()\n",
        "        self.critic_target = self.build_model()\n",
        "        self.critic_online.set_weights(self.critic_target.get_weights())\n",
        "\n",
        "    def build_model(self):\n",
        "        input_a = Input(shape = (self.state_dimensions, ))\n",
        "        input_b = Input(shape = (2*self.action_dimensions, ))\n",
        "        input = Concatenate(axis = -1)([input_a, input_b])\n",
        "        x = Dense(60, activation = 'relu')(input)\n",
        "        x = Dense(16, activation = 'relu')(x)\n",
        "        output = Dense(1)(x)\n",
        "        model = keras.Model(inputs=[input_a, input_b], outputs = output)\n",
        "        model.compile(loss='mse', optimizer = keras.optimizers.Adam(learning_rate = 0.001))\n",
        "        # model.summary()\n",
        "        return model\n",
        "\n",
        "    def save(self):\n",
        "        self.critic_online.save(path_base + 'critic_online.h5')\n",
        "        self.critic_target.save(path_base + 'critic_target.h5')\n",
        "\n",
        "    def load(self):\n",
        "        self.critic_online = keras.models.load_model(path_base + 'critic_online.h5')\n",
        "        self.critic_target = keras.models.load_model(path_base + 'critic_target.h5')\n",
        "\n",
        "    def get_qvalues(self, state_array, action_array, online=True):\n",
        "        if online:\n",
        "            return self.critic_online([convert(state_array), convert(action_array)])\n",
        "        else:\n",
        "            return self.critic_target([convert(state_array), convert(action_array)])\n",
        "\n",
        "    def call(self, state_tensor, action_tensor):\n",
        "        return self.critic_online([state_tensor, action_tensor])\n",
        "    \n",
        "    def merge(self):\n",
        "        self.critic_target.set_weights(self.tau*np.array(self.critic_online.get_weights())\n",
        "                                                                    + (1-self.tau)*np.array(self.critic_target.get_weights()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS69nBDyQ-I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, params, train = True, resume = True):\n",
        "        self.train = train\n",
        "        self.actor = Actor(params)\n",
        "        self.critic = Critic(params)\n",
        "        self.buffer = ReplayMemory(params[\"buffer_size\"])\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.discount = params[\"discount\"]\n",
        "        self.action_range = params[\"output_range\"]\n",
        "        self.save_frequency = params[\"save_frequency\"]\n",
        "        self.batch_size = params[\"batch_size\"]\n",
        "        self.optimizer = params[\"actor_optimizer\"]\n",
        "        self.cap = params['cap']\n",
        "        self.num_steps = 0\n",
        "        self.noise_func =  OrnsteinUhlenbeckActionNoise(mu=np.zeros(2*params[\"action_dimensions\"]))\n",
        "        if resume:\n",
        "            self.load()\n",
        "        \n",
        "    def agent_start(self, observation):\n",
        "        action = self.actor.get_action(observation)\n",
        "        if self.train:\n",
        "            action = self.clip_action(action + self.noise_func())\n",
        "        else:\n",
        "            action = self.clip_action(action)\n",
        "\n",
        "        self.prev_state = observation\n",
        "        self.prev_action = action\n",
        "        return action\n",
        "\n",
        "    def clip_action(self, action):\n",
        "        action = np.clip(action, 0, self.action_range)\n",
        "        sum = np.sum(action, axis = -1, keepdims = True)\n",
        "        action = action/sum*np.minimum(sum, self.cap)\n",
        "        return action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        if self.train:\n",
        "            replay = (self.prev_state, self.prev_action, reward, observation)\n",
        "            self.buffer.append(replay)\n",
        "        action = self.actor.get_action(observation)\n",
        "        if self.train:\n",
        "            action = self.clip_action(action + self.noise_func())\n",
        "            self.run()\n",
        "        self.prev_action = action\n",
        "        self.prev_state = observation\n",
        "        return self.prev_action \n",
        "    \n",
        "    def save(self):\n",
        "        self.actor.save()\n",
        "        self.critic.save()\n",
        "        data = (self.buffer, self.num_steps, self.noise_func)\n",
        "        with open (path_base + 'auxiliary.pkl', 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "    def load(self):\n",
        "        self.actor.load()\n",
        "        self.critic.load()\n",
        "        with open (path_base + 'auxiliary.pkl', 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "        self.buffer, self.num_steps, self.noise_func = data\n",
        "    \n",
        "    def run(self):\n",
        "        self.num_steps += 1\n",
        "        size = min(self.batch_size, self.buffer.size)\n",
        "        batch = self.buffer.sample(size)\n",
        "\n",
        "        prev_states = np.array([x[0] for x in batch]).reshape((-1, self.state_dimensions))\n",
        "        prev_actions = np.array([x[1] for x in batch]).reshape((-1, 2*self.action_dimensions))\n",
        "        rewards = np.array([x[2] for x in batch]).reshape((-1, 1))\n",
        "        states = np.array([x[3] for x in batch]).reshape((-1, self.state_dimensions))\n",
        "        actions = self.actor.get_action(states)\n",
        "        q_values = self.critic.get_qvalues(states, actions, False)\n",
        "        q_values += self.discount*rewards\n",
        "        self.critic.critic_online.fit([states, actions], q_values, epochs = 1, verbose=0)\n",
        "\n",
        "        prev_state_tensor = convert(prev_states)\n",
        "        prev_action_tensor = convert(prev_actions)\n",
        "        \n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            tape.watch(prev_action_tensor)\n",
        "            value = self.critic.call(prev_state_tensor, prev_action_tensor)\n",
        "            action = self.actor.actor(prev_state_tensor)\n",
        "        gradient = -tape.gradient(value, prev_action_tensor)\n",
        "        gradient = tf.cast(gradient, tf.float32)\n",
        "        gradient_actor = tape.gradient(action, self.actor.actor.trainable_weights, gradient)\n",
        "        gradient_actor = list(np.array(gradient_actor)/size)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradient_actor, self.actor.actor.trainable_weights))\n",
        "        self.critic.merge()\n",
        "\n",
        "        if self.num_steps % self.save_frequency == 0:\n",
        "            self.save()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewfr3d3HQ_5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AGENT_PARAMS = {\n",
        "\t\"output_range\": 1,\n",
        "\t\"state_dimensions\": 11,\n",
        "\t\"action_dimensions\": 2,\n",
        "\t\"critic_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
        "\t\"actor_optimizer\": tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "\t\"batch_size\": 64,\n",
        "\t\"buffer_size\":100000,\n",
        "\t\"discount\": 0.99,\n",
        "\t\"tau\": 0.001,\n",
        "\t\"save_frequency\": 5000,\n",
        "\t'cap' : 0.9,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BBct9SwRCJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = ['/content/drive/My Drive/AAPL.csv','/content/drive/My Drive/MSFT.csv']\n",
        "env = create_stock_env(files)\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "agent = Agent(AGENT_PARAMS, train = True,resume = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt9u_BcKREBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent, env, profits, actions, balances, shares):\n",
        "\n",
        "    epochs = 40\n",
        "    steps_per_epoch = 2000\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        cumm_profit = 0\n",
        "        observation, info = env.reset()\n",
        "        shares[epoch, 0, :] = info['shares_held']*info['shares_normal']\n",
        "        balances[epoch, 0] = info['balance']*info['balance_normal']\n",
        "        action = agent.agent_start(observation)\n",
        "        actions[epoch, 0, :] = action\n",
        "\n",
        "        for i in tqdm_notebook(range(steps_per_epoch)):\n",
        "\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            shares[epoch, i+1, :] = info['shares_held']*info['shares_normal']\n",
        "            balances[epoch, i+1] = info['balance']*info['balance_normal']\n",
        "            cumm_profit += reward\n",
        "            profits[epoch, i] = cumm_profit\n",
        "            if done:\n",
        "                break\n",
        "            action = agent.agent_step(reward, observation)\n",
        "            actions[epoch, i+1, :] = action\n",
        "\n",
        "        print('Completed epoch' + str(epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHd3Ia9aLz0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100,
          "referenced_widgets": [
            "86c6302fc52444878bfac1a14a816949",
            "249e0ffddb02442da42324b01dd24871",
            "be21174636584de79bb117802cd8f11d",
            "0e3a891c284e45058e731f32cfde6b58",
            "427a2c5bcde14c9ca2a3ac8fab5cba52",
            "24d9f541380544b3a3d5327d6763f3e7",
            "8be1411bf27b42d49ad8705001f714a4",
            "9fa16c33bfc2447a8d14a9059baf66eb"
          ]
        },
        "outputId": "3ce17674-aaad-4273-db95-7ba7dff7b10f"
      },
      "source": [
        "steps_per_epoch = 2000\n",
        "profits = np.zeros((200, steps_per_epoch))\n",
        "actions = np.zeros((200, steps_per_epoch+1, 2*agent.action_dimensions))\n",
        "shares = np.zeros((200, steps_per_epoch+1, agent.action_dimensions))\n",
        "balances = np.zeros((200, steps_per_epoch+1))\n",
        "train(agent, env, profits, actions, balances, shares)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86c6302fc52444878bfac1a14a816949",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcLVB4jtR81T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(profits[i])\n",
        "\n",
        "plt.legend(list(range(0, 40, 5)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFs2NZruSUbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(shares[i, :][0])\n",
        "\n",
        "plt.legend(list(range(0, 40, 5))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ep1chtITFSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(balances[i])(list(range(0, 40, 5)))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMWI3L6TSsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Action Plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xanZGhfQMa7T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(values):\n",
        "    num = len(values)\n",
        "    max_steps = env.max_steps\n",
        "    profits = np.zeros((num, max_steps))\n",
        "    balances = np.zeros((num, max_steps + 1))\n",
        "    shares = np.zeros((num, max_steps+1, agent.action_dimensions))\n",
        "    actions = np.zeros((num, max_steps+1, agent.action_dimensions*2))\n",
        "\n",
        "    for count, val in enumerate(values):\n",
        "        profit = 0\n",
        "        env = create_stock_env(files, train = False)\n",
        "        agent = Agent(AGENT_PARAMS, train = False, resume = True)\n",
        "        observation, info = env.reset(initial_balance = val)\n",
        "        balances[count][0] = info['balance']*info['balance_normal']\n",
        "        shares[count][0] = info['shares_held']*info['shares_normal']\n",
        "        action = agent.agent_start(observation)\n",
        "        actions[count][0] = action\n",
        "\n",
        "            for i in tqdm_notebook(range(max_steps)):\n",
        "                observation, reward, done, info = env.step(action)\n",
        "                profit += reward\n",
        "                profits[count][i] = profit\n",
        "                balances[count][i+1] = info['balance']*info['balance_normal']\n",
        "                shares[count][i+1] = info['shares_held']*info['shares_normal']\n",
        "                if done:\n",
        "                    break\n",
        "                action = agent.agent_step(reward, observation)\n",
        "                actions[count][i+1] = action\n",
        "              \n",
        "            print('Completed' + str(count) + 'values')\n",
        "\n",
        "    return profits, balances, shares, actions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cscoUT_4RS8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "values = [1000, 5000, 10000, 20000, 50000]\n",
        "profits, balances, shares, actions = test(values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDpKfz_fUF0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(profits[i])\n",
        "\n",
        "plt.legend(list(range(0, 40, 5)))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8butb_rUMOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(shares[i, :][0])\n",
        "\n",
        "plt.legend(list(range(0, 40, 5))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRxi43kVUQ4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, 40, 5):\n",
        "    plt.plot(balances[i])(list(range(0, 40, 5)))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCRprSYCUT2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Action Plot"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}