{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "from gym import spaces\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "import random\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Lambda, Activation, LSTM\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow import convert_to_tensor as convert\n",
    "import pickle\n",
    "COLAB = False\n",
    "if not COLAB:\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "path_base = 'models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, df, params, train = True):\n",
    "        super(StockEnv,self).__init__()\n",
    "        \n",
    "        self.min_brokerage = params['min_brokerage']\n",
    "        self.brokerage_rate = params['brokerage_rate']\n",
    "        self.df = df\n",
    "        self.state_dimensions = 6\n",
    "        self.shares_normal = params['shares_normal']\n",
    "        self.train = train\n",
    "\n",
    "        self.max_steps = len(self.df.loc[:, \"Open\"])\n",
    "        self.action_space = spaces.Box(low = -1, high = 1, shape =  (1, 1), dtype = np.float32)\n",
    "        self.observation_space = spaces.Box(low = 0, high = 1, shape = (1, self.state_dimensions), dtype = np.float32)\n",
    "\n",
    "    def reset(self, initial_balance = 10000, shares_held = None):\n",
    "        self.start_balance = initial_balance \n",
    "        if self.train:\n",
    "            self.current_step = np.random.randint(0, self.max_steps)\n",
    "        else:\n",
    "            self.current_step = 0\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = shares_held\n",
    "        if self.shares_held is None:\n",
    "            self.shares_held = 0\n",
    "        self.current_price = self.get_price()\n",
    "        self.net_worth = self.balance + (self.shares_held*self.current_price)\n",
    "        self.initial_worth = self.net_worth\n",
    "        self.max_net_worth = self.net_worth\n",
    "        self.done = False\n",
    "        self.frame = np.zeros((1, self.state_dimensions))\n",
    "        self.info = {\n",
    "            'current_step' : self.current_step,\n",
    "            'current_price': self.current_price,\n",
    "            'net_worth' : self.net_worth,\n",
    "            'max_net_worth': self.max_net_worth,\n",
    "            'shares_held' : self.shares_held,\n",
    "            'balance' : self.balance,\n",
    "        }\n",
    "        return self.observe()\n",
    "        \n",
    "    def get_price(self):\n",
    "        return np.random.uniform(self.df.loc[self.current_step,\"Low\"], self.df.loc[self.current_step,\"High\"]) \n",
    "      \n",
    "    def observe(self):\n",
    "        self.frame[0, 0:4] = np.array([self.df.loc[self.current_step,'Open'],self.df.loc[self.current_step,'High'],self.df.loc[self.current_step,'Low'],self.df.loc[self.current_step,'Close']])/self.balance\n",
    "        self.frame[0, 4] = self.shares_held/self.shares_normal\n",
    "        self.frame[0, 5] = self.balance/self.start_balance\n",
    "        self.info = {\n",
    "            'current_step' : self.current_step,\n",
    "            'current_price': self.current_price,\n",
    "            'net_worth' : self.net_worth,\n",
    "            'max_net_worth': self.max_net_worth,\n",
    "            'shares_held' : self.shares_held,\n",
    "            'balance' : self.balance\n",
    "        }\n",
    "        return self.frame, self.info\n",
    "    \n",
    "    def update_worth(self, reward):\n",
    "        self.net_worth += reward\n",
    "        self.max_net_worth = max(self.max_net_worth, self.net_worth)\n",
    "\n",
    "    def update_balance(self, action):\n",
    "        self.balance -= action*self.current_price\n",
    "\n",
    "    def update_shares(self, action):\n",
    "        self.shares_held += action\n",
    "\n",
    "    def take_action(self, action):\n",
    "        self.current_price = self.get_price()\n",
    "        max_buyable = self.balance/self.current_price\n",
    "        max_sellable = self.shares_held\n",
    "        if action >= 0:\n",
    "            action *= max_buyable\n",
    "            action = floor(action)\n",
    "        else:\n",
    "            action *= max_sellable\n",
    "            action = ceil(action)\n",
    "            \n",
    "        if self.shares_held == 0 and action < 0:\n",
    "            reward = -self.balance/2 if self.train else 0\n",
    "            print (\"Invallid sell action\")\n",
    "        else:\n",
    "            self.update_balance(action)\n",
    "            self.update_shares(action)\n",
    "            reward = self.balance + (self.shares_held * self.current_price) - self.net_worth\n",
    "            self.update_worth(reward)\n",
    "        return reward\n",
    "            \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps or self.done:\n",
    "            self.done = True\n",
    "            return np.zeros((1, self.state_dimensions)), 0, self.done, self.info\n",
    "\n",
    "        reward = self.take_action(action)\n",
    "        self.done = self.net_worth <= self.initial_worth*0.05\n",
    "        if self.done:\n",
    "            print('snap')\n",
    "        obs, info = self.observe()\n",
    "        return obs, reward, self.done, info\n",
    "    \n",
    "    def render(self, mode='human', close = False):\n",
    "        profit = self.net_worth - self.initial_worth\n",
    "        print('Step: {}'.format(self.current_step))\n",
    "        print('Net Worth: {}'.format(self.net_worth))\n",
    "        print('Profit: {}'.format(profit))\n",
    "        \n",
    "def create_stock_env(location, train=True):\n",
    "    df = pd.read_csv(location).sort_values('Date')\n",
    "    params = {\n",
    "        'num_stocks' : 1,\n",
    "        'min_brokerage' : 30.0,\n",
    "        'shares_normal' : 10000,\n",
    "        'brokerage_rate' : 0.001,\n",
    "    }\n",
    "    return StockEnv(df, params, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = [None] * max_size\n",
    "        self.max_size = max_size\n",
    "        self.index = 0\n",
    "        self.size = 0\n",
    "\n",
    "    def append(self, obj):\n",
    "        self.buffer[self.index] = obj\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        self.index = (self.index + 1) % self.max_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = random.sample(range(self.size), batch_size)\n",
    "        return [self.buffer[index] for index in indices]\n",
    "\n",
    "from collections import deque\n",
    "class ReplayDeque:\n",
    "    def __init__(self, capacity, element_dimensions):\n",
    "        self.deque = deque()\n",
    "        self.capacity = capacity\n",
    "        self.dim = element_dimensions\n",
    "        for _ in range(self.capacity):\n",
    "            self.deque.append(np.zeros((element_dimensions, )))\n",
    "            \n",
    "    def add(self, obj):\n",
    "        copied = np.copy(obj)\n",
    "        self.deque.append(np.squeeze(copied))\n",
    "        self.deque.popleft()\n",
    "        \n",
    "    def get_last(self, duration):\n",
    "        entries = list(self.deque)[-duration:]\n",
    "        shape = list(entries[0].shape)\n",
    "        shape[:0] = [1, len(entries)]\n",
    "        res = np.concatenate(entries).reshape(shape)\n",
    "        return res\n",
    "    def clear(self):\n",
    "        self.__init__(self.capacity, self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params, resume=True, train=True):\n",
    "        self.training = train\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = params[\"decay\"]\n",
    "        self.epsilon_min = params[\"min_epsilon\"]\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.merge_frequency = params[\"merge_frequency\"]\n",
    "        self.save_frequency = params[\"save_frequency\"]\n",
    "        self.replay_length = params[\"replay_length\"]\n",
    "        self.num_actions = params[\"actions\"]\n",
    "        self.state_dimensions = params[\"state_dimensions\"]\n",
    "        self.batch_size = params[\"batch_size\"]\n",
    "        self.optimizer = params[\"optimizer\"]\n",
    "        self.experience_memory = params[\"memory\"]\n",
    "        self.buffer = ReplayMemory(self.experience_memory)\n",
    "        self.past_states = ReplayDeque(self.replay_length, self.state_dimensions)\n",
    "        self.count = 0\n",
    "        self.game = 0\n",
    "        self.input_shape = (self.replay_length, self.state_dimensions)\n",
    "        self.q_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        if resume:\n",
    "            self.load_weights()\n",
    "        \n",
    "    def clear_memory(self):\n",
    "        self.past_states.clear()\n",
    "    def merge_networks(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        model.add(tf.keras.layers.LSTM(24, activation='relu', \n",
    "                        input_shape=self.input_shape))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu', \n",
    "                         kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, activation='linear', kernel_initializer=initializer))\n",
    "        model.compile(loss='mse', optimizer=self.optimizer)\n",
    "        return model\n",
    "    \n",
    "    def agent_start(self, observation):\n",
    "        self.past_states.add(observation)\n",
    "        state = (self.past_states.get_last(self.replay_length))\n",
    "        q_values = np.squeeze(self.q_network(convert(state)).numpy())\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        \n",
    "        if np.random.random() < self.epsilon and self.training:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action \n",
    "        return (action-10)/10\n",
    "    \n",
    "    def agent_step(self, reward, observation):\n",
    "        self.past_states.add(observation)\n",
    "        state = (self.past_states.get_last(self.replay_length))\n",
    "        self.count += 1\n",
    "\n",
    "        q_values = np.squeeze(self.q_network(convert(state)).numpy())\n",
    "        relay = (self.prev_state, self.prev_action,  reward, state, 0)\n",
    "        self.buffer.append(relay)\n",
    "        \n",
    "        if np.random.random() < self.epsilon and self.training:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values)\n",
    "        self.prev_state = state\n",
    "        self.prev_action = action \n",
    "        if self.training:\n",
    "            self.train(self.batch_size)\n",
    "        \n",
    "        return (action-10)/10\n",
    "             \n",
    "    def save_weights(self):\n",
    "        self.q_network.save_weights(path_base + \"main.h5\")\n",
    "        self.target_network.save_weights(path_base + \"target.h5\")\n",
    "        data = (self.buffer, self.count)\n",
    "        with open (path_base + 'auxiliary.pkl', 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "               \n",
    "    def load_weights(self):\n",
    "        self.q_network.load_weights(path_base + \"main.h5\")\n",
    "        self.target_network.load_weights(path_base + \"target.h5\")\n",
    "        with open (path_base + 'auxiliary.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        self.buffer, self.count = data\n",
    "        \n",
    "    def train(self, count):\n",
    "        size = min(count, self.buffer.size)\n",
    "        batch = self.buffer.sample(size)\n",
    "        input_tensor = np.array([state[0] for state, action, reward, future, terminated in batch])\n",
    "        output_tensor = self.q_network(convert(input_tensor)).numpy()\n",
    "        future_input_tensor = np.array([future[0] for state, action, reward, future, terminated in batch])\n",
    "        future_out = self.target_network(convert(future_input_tensor)).numpy()\n",
    "        for count, (state, action, reward, future, terminated) in enumerate(batch):\n",
    "            target = output_tensor[count]\n",
    "            updated = reward\n",
    "            if not terminated:\n",
    "                target_vals = future_out[count]\n",
    "                updated += self.discount*(target_vals[np.argmax(target)])\n",
    "                \n",
    "            target[action] = updated\n",
    "            output_tensor[count] = target \n",
    "        \n",
    "        input_tensor = np.array(input_tensor)\n",
    "        output_tensor = np.array(output_tensor)\n",
    "        self.q_network.fit(input_tensor, output_tensor, epochs=1, verbose=0)\n",
    "        if self.count%self.merge_frequency == 0:\n",
    "            self.merge_networks()\n",
    "            \n",
    "        if self.count%self.save_frequency == 0:\n",
    "            self.save_weights()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 24)                2976      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 3,626\n",
      "Trainable params: 3,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "files = 'data/PCG.csv'\n",
    "env = create_stock_env(files)\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "params = {\"state_dimensions\":6, \"decay\":0.995, \"batch_size\":32, \"merge_frequency\": 10000, \"replay_length\":10, \"min_epsilon\": 0.1, \"save_frequency\": 5000, \"discount\": 0.95,  \"actions\": 2, \"optimizer\": optimizer, \n",
    "          \"memory\": 70000}\n",
    "agent = Agent(params, False)\n",
    "print (agent.q_network.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, epochs, profits, balances, shares, actions, steps_per_epoch):\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        cumm_profit = 0\n",
    "        observation, info = env.reset(shares_held=100)\n",
    "        shares[epoch, 0] = info['shares_held']\n",
    "        balances[epoch, 0] = info['balance']\n",
    "        action = agent.agent_start(observation)\n",
    "        actions[epoch, 0] = action\n",
    "\n",
    "        for i in tqdm_notebook(range(steps_per_epoch)):\n",
    "#             print (agent.past_states.deque)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "#             print (\"Observation\")\n",
    "#             print (observation)\n",
    "#             print (\"ok\")\n",
    "            shares[epoch, i+1] = info['shares_held']\n",
    "            balances[epoch, i+1] = info['balance']\n",
    "            cumm_profit += reward\n",
    "            print (\"Action \" + str(action))\n",
    "            print (\"Profit \" + str(cumm_profit))\n",
    "            profits[epoch, i] = cumm_profit\n",
    "            if done:\n",
    "                print(\"the end\")\n",
    "                break\n",
    "            action = agent.agent_step(reward, observation)\n",
    "            actions[epoch, i+1] = action\n",
    "\n",
    "        agent.clear_memory()\n",
    "        print('Completed epoch' + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db15921d124e4620a9ddb2f981784de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action -0.9\n",
      "Profit -4.730298593845873\n",
      "Action -0.9\n",
      "Profit -4.927391646611795\n",
      "Action -0.9\n",
      "Profit -5.420264626227436\n",
      "Action -0.9\n",
      "Profit -6.071207069069715\n",
      "Action -0.9\n",
      "Profit -5.684112148190252\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -1.0\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "Action -0.9\n",
      "Profit -5.946131530450657\n",
      "\n",
      "Completed epoch0\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "steps_per_epoch = 20\n",
    "profits = np.zeros((epochs, steps_per_epoch+1))\n",
    "balances = np.zeros((epochs, steps_per_epoch+1))\n",
    "shares = np.zeros((epochs, steps_per_epoch+1))\n",
    "actions = np.zeros((epochs, steps_per_epoch+1))\n",
    "train(agent, env, epochs, profits, balances, shares, actions, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(6):\n",
    "    plt.plot(profits[_])\n",
    "print (profits[0])\n",
    "plt.legend(list(range(10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(values):\n",
    "    num = len(values)\n",
    "    env = create_stock_env(files, train = False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "    params = {\"state_dimensions\":6, \"decay\":0.995, \"batch_size\":32, \"merge_frequency\": 10000, \"replay_length\":10, \"min_epsilon\": 0.1, \"save_frequency\": 5000, \"discount\": 0.95,  \"actions\": 20, \"optimizer\": optimizer, \n",
    "              \"memory\": 70000}\n",
    "    agent = Agent(params, resume = True, train = False)\n",
    "    print(agent.num_steps)\n",
    "    max_steps = env.max_steps\n",
    "    profitst = np.zeros((num, max_steps+1))\n",
    "    balancest = np.zeros((num, max_steps + 1))\n",
    "    sharest = np.zeros((num, max_steps+1))\n",
    "    actionst = np.zeros((num, max_steps+1))\n",
    "    worthst = np.zeros((num, max_steps+1))\n",
    "    for count, val in enumerate(values):\n",
    "        profit = 0\n",
    "        profitst[count][0] = profit\n",
    "        observation, info = env.reset(initial_balance = val)\n",
    "        balancest[count][0] = info['balance']\n",
    "        print(info['balance'])\n",
    "        sharest[count][0] = info['shares_held']\n",
    "        worthst[count][0] = info['net_worth']\n",
    "        print(info['shares_held'])\n",
    "        action = agent.agent_start(observation)\n",
    "        actionst[count][0] = action\n",
    "\n",
    "        for i in tqdm_notebook(range(max_steps)):\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            profit += reward\n",
    "            profitst[count][i+1] = profit\n",
    "            balancest[count][i+1] = info['balance']\n",
    "            sharest[count][i+1] = info['shares_held']\n",
    "            worthst[count][i+1] = info['net_worth']\n",
    "            if done:\n",
    "                print (\"ober\")\n",
    "                break\n",
    "            action = agent.agent_step(reward, observation)\n",
    "            actionst[count][i+1] = action\n",
    "              \n",
    "        print('Completed' + str(count) + 'values')\n",
    "\n",
    "    print(env.net_worth)\n",
    "    print(env.balance)\n",
    "    print(env.shares_held)\n",
    "    return profitst, balancest, sharest, actionst, worthst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [10000]\n",
    "profitst, balancest, sharest, actionst, worthst = test(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
