{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, tiles3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSpaceTiler:\n",
    "    def __init__(self, size, tilings, num_tiles, x_min, x_max, y_min, y_max):\n",
    "        self.iht = tiles3.IHT(size)\n",
    "        self.tilings = tilings\n",
    "        self.num_tiles = num_tiles\n",
    "        self.x_transform = lambda x: ((x - x_min)*self.num_tiles)/(x_max - x_min)\n",
    "        self.y_transform = lambda y: ((y - y_min)*self.num_tiles)/(y_max - y_min)\n",
    "    \n",
    "    def get_encoding(self, x, y):\n",
    "        return np.array(tiles3.tiles(self.iht, self.tilings, [self.x_transform(x), self.y_transform(y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params, train=False):\n",
    "        self.alpha_r = params.get(\"alpha_r\")\n",
    "        self.tilings = params.get(\"tilings\")\n",
    "        self.num_tiles = params.get(\"num_tiles\")\n",
    "        self.action_values = params.get(\"actions\")\n",
    "        self.iht_size = params.get(\"iht_size\")\n",
    "        self.alpha_w = params.get(\"alpha_w\")/self.tilings\n",
    "        self.alpha_theta =  params.get(\"alpha_t\")/self.tilings\n",
    "        self.tiler = StateSpaceTiler(self.iht_size,self.tilings, self.num_tiles, -np.pi, np.pi, -2*np.pi, 2*np.pi)\n",
    "        self.weights = np.zeros((self.iht_size, ))\n",
    "        self.policy = np.zeros((len(self.action_values), self.iht_size))\n",
    "        self.reward_mean = 0\n",
    "        if not train:\n",
    "            self.load()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return self.reward_mean\n",
    "    def save(self):\n",
    "        np.save('policy.npy', self.policy)\n",
    "        np.save('weights.npy', self.weights)\n",
    "        \n",
    "    def load(self):\n",
    "        self.weights = np.load('weights.npy')\n",
    "        self.policy  = np.load('policy.npy')\n",
    "    \n",
    "    def softmax_dist(self, state):\n",
    "        return np.sum(self.policy[:, state], axis=-1)\n",
    "    \n",
    "    def process(self, position):\n",
    "        sign = position/abs(position)\n",
    "        position = abs(position) % (2*np.pi)\n",
    "        if position > np.pi:\n",
    "            position -= 2*np.pi\n",
    "        return position*sign\n",
    "    \n",
    "    def softmax_prob(self, state):\n",
    "        dist = self.softmax_dist(state)\n",
    "        max_val = np.max(dist)\n",
    "        dist -= max_val\n",
    "        p = np.exp(dist)\n",
    "        return p/(np.sum(p))\n",
    "    def softmax_action(self, state):\n",
    "        return np.random.choice(len(self.action_values), p = self.softmax_prob(state))\n",
    "        \n",
    "    def agent_init(self, env_state):\n",
    "        position, self.velocity = env_state\n",
    "        self.position = self.process(position)\n",
    "        self.previous_state = self.tiler.get_encoding(self.position, self.velocity)\n",
    "        self.previous_action = self.softmax_action(self.previous_state)\n",
    "        \n",
    "        return self.action_values[self.previous_action] \n",
    "\n",
    "    def get_value(self, state):\n",
    "        return (np.sum(self.weights[state]))\n",
    "    \n",
    "    def agent_step(self, env_state, reward):\n",
    "        position, self.velocity = env_state\n",
    "        self.position = self.process(position)\n",
    "        current_state = self.tiler.get_encoding(self.position, self.velocity)\n",
    "        td_error = reward - self.reward_mean + self.get_value(current_state) - self.get_value(self.previous_state)\n",
    "        self.reward_mean = (self.reward_mean + self.alpha_r*td_error)/(1+self.alpha_r)\n",
    "        \n",
    "        self.weights[self.previous_state] += self.alpha_w*td_error\n",
    "        prob_dist = self.softmax_prob(self.previous_state)\n",
    "        for action in range(len(self.action_values)):\n",
    "            prob_scale = -prob_dist[action]\n",
    "            if action == self.previous_action:\n",
    "                prob_scale = 1  - prob_dist[action]\n",
    "            self.policy[action][self.previous_state] += self.alpha_theta*td_error*prob_scale\n",
    "        \n",
    "        self.previous_state = current_state\n",
    "        self.previous_action = self.softmax_action(self.previous_state)\n",
    "        \n",
    "        return self.action_values[self.previous_action]\n",
    "    \n",
    "    def play(self, env_state):\n",
    "        position, velocity = env_state\n",
    "        position = self.process(position)\n",
    "        current_state = self.tiler.get_encoding(position, velocity)\n",
    "        dist = self.softmax_prob(current_state)\n",
    "        \n",
    "        return self.action_values[np.argmax(dist)]\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_params = {\n",
    "    \"alpha_r\": 2**-6,\n",
    "    \"tilings\": 32,\n",
    "    \"num_tiles\": 8,\n",
    "    \"actions\": [-1, 0, 1],\n",
    "    \"iht_size\": 4096, \n",
    "    \"alpha_w\": 0.001,\n",
    "    \"alpha_t\": 2**(-5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(agent_params, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, agent_params, env):\n",
    "    num_steps = 400000\n",
    "    env.reset()\n",
    "    action = agent.agent_init(env.state)\n",
    "\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    for _ in range(num_steps):\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        env.render()\n",
    "        action = agent.agent_step(env.state, reward)\n",
    "        if _ % 10000 == 0:\n",
    "            print (\"Cycle {0} Value is {1}\".format(_//10000, agent.get_reward()))\n",
    "            print (\"State is Position: {0}, Velocity {1}\".format(agent.position, agent.velocity))\n",
    "            x_points.append(_/1000)\n",
    "            y_points.append(agent.get_reward())\n",
    "    plt.plot(x_points, y_points)\n",
    "    plt.show()\n",
    "    agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, env):\n",
    "    env.reset()\n",
    "    play_steps = 10000000\n",
    "    for _ in range(play_steps):\n",
    "        action = agent.play(env.state)\n",
    "        env.step([action])\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle 0 Value is -3.08919222088632\n",
      "State is Position: 1.2686020703193848, Velocity 1.2167422855387202\n",
      "Cycle 1 Value is -1.5634416053467228\n",
      "State is Position: -0.4308779475135509, Velocity 0.5567798140435356\n",
      "Cycle 2 Value is -2.249385520332149\n",
      "State is Position: 1.948679662198586, Velocity -6.69743617746083\n",
      "Cycle 3 Value is -2.2100983701477346\n",
      "State is Position: -2.763140013970556, Velocity 8.0\n",
      "Cycle 4 Value is -2.5022403634563326\n",
      "State is Position: 1.4924107523752284, Velocity -2.274765582441436\n",
      "Cycle 5 Value is -2.4868818561670367\n",
      "State is Position: 1.3438458033013134, Velocity -1.1266808591840085\n",
      "Cycle 6 Value is -2.8359872002497735\n",
      "State is Position: -1.48864246252856, Velocity -2.0412946982671283\n",
      "Cycle 7 Value is -2.813736083308669\n",
      "State is Position: -2.155107076741217, Velocity -4.416853511945435\n",
      "Cycle 8 Value is -2.699379743043648\n",
      "State is Position: 2.098104071915081, Velocity -4.082452746938674\n",
      "Cycle 9 Value is -2.769517654841018\n",
      "State is Position: -1.3713662048361641, Velocity -1.9684650824412024\n",
      "Cycle 10 Value is -2.6687754214667736\n",
      "State is Position: -1.3693086504063388, Velocity -2.844034992194434\n",
      "Cycle 11 Value is -2.601611924038769\n",
      "State is Position: -0.9674375903693146, Velocity 0.2179279242200095\n",
      "Cycle 12 Value is -2.470060479078657\n",
      "State is Position: -1.0942737747592162, Velocity 1.8397106570504098\n",
      "Cycle 13 Value is -2.455731586579897\n",
      "State is Position: 1.8918179089737848, Velocity 4.481103002987702\n",
      "Cycle 14 Value is -2.2860952258196168\n",
      "State is Position: -0.8612073011233221, Velocity 1.9910319066682105\n",
      "Cycle 15 Value is -1.8912842380523762\n",
      "State is Position: -0.5440114953259254, Velocity 2.288017716311733\n",
      "Cycle 16 Value is -1.101869247461926\n",
      "State is Position: 0.5565618433180717, Velocity 1.378894397972589\n",
      "Cycle 17 Value is -1.8562857340883685\n",
      "State is Position: -1.044927920870414, Velocity 4.563360004152402\n",
      "Cycle 18 Value is -1.7432779093939317\n",
      "State is Position: -0.8430304309424272, Velocity 3.7764433065899667\n",
      "Cycle 19 Value is -1.2752789478413975\n",
      "State is Position: 1.4997732340807488, Velocity 5.107253380890759\n",
      "Cycle 20 Value is -1.233065334704008\n",
      "State is Position: 0.8580306202022427, Velocity 2.5146574689554226\n",
      "Cycle 21 Value is -0.8515102025924693\n",
      "State is Position: 0.6153439203268434, Velocity 1.420955028163407\n",
      "Cycle 22 Value is -1.5838283549772878\n",
      "State is Position: 2.922088621262276, Velocity 8.0\n",
      "Cycle 23 Value is -0.03104381273867668\n",
      "State is Position: 0.14673550645943578, Velocity 0.015588440010423407\n",
      "Cycle 24 Value is -0.03824501355750384\n",
      "State is Position: 0.1947598691814818, Velocity 0.16161910407177127\n",
      "Cycle 25 Value is -0.009157075804398106\n",
      "State is Position: 0.08401747840044038, Velocity 0.04053671489469157\n",
      "Cycle 26 Value is -0.9916152062985641\n",
      "State is Position: -0.23680591131177664, Velocity 1.5741961800095081\n",
      "Cycle 27 Value is -0.0013872616961524181\n",
      "State is Position: 0.11947471263098919, Velocity -0.06639671000121117\n",
      "Cycle 28 Value is -0.009345515395849137\n",
      "State is Position: 0.06404114263127525, Velocity 0.16054822106905314\n",
      "Cycle 29 Value is -0.002270070221700462\n",
      "State is Position: 0.09293869474083749, Velocity -0.01595018164636426\n",
      "Cycle 30 Value is 0.0012906387710458378\n",
      "State is Position: 0.07425860038947718, Velocity -6.667084091648012e-05\n",
      "Cycle 31 Value is -0.0028773546918588588\n",
      "State is Position: 0.08531609072324642, Velocity 0.02243907950300189\n"
     ]
    }
   ],
   "source": [
    "train(agent, agent_params, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
