{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AptN_88ZggNd",
        "colab_type": "text"
      },
      "source": [
        "### Deep Deterministic Policy Gradient for Continuous Pendulum Control "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICc1SN_5ggNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tqdm\n",
        "import gym\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwV1V287ggNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque\n",
        "import random\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size):\n",
        "        self.size = buffer_size\n",
        "        self.buffer = deque()\n",
        "        \n",
        "    def add(self, data_tuple):\n",
        "        self.buffer.append(data_tuple)\n",
        "        if len(self.buffer) > self.size:\n",
        "            self.buffer.poplef()\n",
        "            \n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.buffer, min(len(self.buffer), sample_size))\n",
        "\n",
        "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
        "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
        "class OrnsteinUhlenbeckActionNoise:\n",
        "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
        "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l2F_tEoggNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor:\n",
        "    def __init__(self, params):\n",
        "        self.output_range = params[\"output_range\"]\n",
        "        self.hidden_layers = params[\"hidden_layers\"]\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.actor = self.model()\n",
        "        \n",
        "    def model(self):\n",
        "        inputs = Input(shape=(1, self.state_dimensions))\n",
        "        x = BatchNormalization()(inputs)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = Dense(layer, activation='relu')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "        x = Dense(self.output_dimensions, activation=self.activation)(x)\n",
        "        x = Lambda(lambda x: x*self.output_range)(x)\n",
        "        model = Model(inputs = inputs, outputs = x)\n",
        "        return model\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        return self.actor.predict(np.array([state]))[0][0]\n",
        "    \n",
        "class Critic:\n",
        "    def __init__(self, params):\n",
        "        self.hidden_layers = params[\"hidden_layers\"]\n",
        "        self.state_dimensions = params[\"state_dimensions\"]\n",
        "        self.action_dimensions = params[\"action_dimensions\"]\n",
        "        self.optimizer = params[\"optimizer\"]\n",
        "        self.critic_online = self.model()\n",
        "        self.critic_target = self.model()\n",
        "\n",
        "\n",
        "    def model(self):\n",
        "        input_a = Input(shape = (1, self.state_dimensions))\n",
        "        input_b = Input(shape = (1, self.action_dimensions))\n",
        "        input_layer = concatenate([input_a, input_b], axis=-1)\n",
        "        x = BatchNormalization()(input_layer)\n",
        "        for layer in self.hidden_layers:\n",
        "            x = Dense(layer, activation='relu')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "        x = Dense(1, activation='linear')(x)\n",
        "        model = Model(inputs=[input_a, input_b], outputs = x)\n",
        "        model.compile(loss='mse', optimizer=self.optimizer)\n",
        "        return model\n",
        "    \n",
        "    def get_qvalues(self, state_tensor, action_tensor, online=True):\n",
        "        return (self.critic_online(state_tensor, action_tensor) if online else self.critic_target(state_tensor, action_tensor))\n",
        "    \n",
        "    def merge_networks(self, tau):\n",
        "        self.critic_target.set_weights(tau*np.array(self.critic_online.get_weights())\n",
        "                                                                    + (1-tau)*np.array(self.critic_target.get_weights()))\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuTDUt9ZggNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, params):\n",
        "        self.actor = Actor(params)\n",
        "        self.critic = Critic(params)\n",
        "        self.buffer = ReplayBuffer(params[\"buffer_size\"])\n",
        "        self.epsilon = 1\n",
        "        self.epsilon_decay = params[\"decay\"]\n",
        "        self.epsilon_min = params[\"min_epsilon\"]\n",
        "        self.discount = params[\"discount\"]\n",
        "        self.merge_frequency = params[\"merge_frequency\"]\n",
        "        self.save_frequency = params[\"save_frequency\"]\n",
        "        self.batch_size = params[\"batch_size\"]\n",
        "        self.optimizer = params[\"optimizer\"]\n",
        "        self.tau = params[\"tau\"]\n",
        "        self.game = 0\n",
        "        self.noise_func =  OrnsteinUhlenbeckActionNoise(mu=np.zeros(params[\"action_dimensions\"]))\n",
        "        \n",
        "    def agent_start(self, observation):\n",
        "        action = self.actor.get_action(observation) + self.noise_func()\n",
        "        self.prev_state = observation\n",
        "        self.prev_action = action\n",
        "        return action \n",
        "    def agent_step(self, reward, observation):\n",
        "        relay = (self.prev_state, self.prev_action, reward, observation)\n",
        "        self.buffer.add(relay)\n",
        "        self.prev_state = observation\n",
        "        self.prev_action = self.actor.get_action(observation) + self.noise_func()\n",
        "        self.train(self.batch_size)\n",
        "\n",
        "        return self.prev_action \n",
        "\n",
        "\n",
        "    def train(self, sample_size):\n",
        "        batch = self.buffer.sample(sample_size)\n",
        "        for prev_state, prev_action, reward, state in batch:\n",
        "            state_action = np.array([[self.actor.get_action(state)]])\n",
        "            action_tensor = tf.Variable(shape = (1, 1), initial_value = state_action)\n",
        "            input_vector = state.reshape((1, state.shape[0]))\n",
        "            input_tensor = tf.Variable(shape=input_vector.shape, initial_value = input_vector)\n",
        "            output = tf.squeeze(self.critic.get_qvalues(input_tensor, action_tensor, False)).numpy()\n",
        "            output = reward + self.discount*output \n",
        "\n",
        "            self.critic.critic_online.fit([[input_vector], [state_action]], [[output]])\n",
        "\n",
        "            previous_state = previous_state.reshape((1, prev_state.shape[0]))\n",
        "            prev_state_tensor = tf.Variable(shape=previous_state.shape, initial_value = previous_state)\n",
        "            prev_action_tensor = tf.Variable(shape = (1, 1), initial_value = [[prev_action]])\n",
        "            \n",
        "            with tf.GradientTape() as g:\n",
        "                g.watch(prev_action_tensor) \n",
        "                g.watch(prev_state_tensor)\n",
        "                value = self.critic.get_qvalues(prev_state_tensor, prev_action_tensor)\n",
        "                action = self.actor.actor(prev_state_tensor)\n",
        "            \n",
        "            gradient = tf.squeeze(g.gradients(value, prev_action_tensor)).numpy()\n",
        "            gradient_actor = g.gradients(action, self.actor.actor.trainable_weights).numpy()\n",
        "            gradient_actor *= -gradient\n",
        "            \n",
        "            self.optimizer.apply_gradients(zip(gradient_actor, self.actor.actor.trainable_weights))\n",
        "\n",
        "        self.actor.merge_networks(self.tau)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgtPQKWidTnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make('Pendulum-v0')\n",
        "\n",
        "ITERATIONS = 400000\n",
        "render = True\n",
        "pbar = tqdm(desc=\"Game Step: \", total=ITERATIONS)\n",
        "\n",
        "action = agent.agent_start(env.reset())\n",
        "observation, reward, done, info = env.step(action)\n",
        "game_reward = 0\n",
        "\n",
        "for _ in range(ITERATIONS):\n",
        "    action, value = agent.agent_step(reward, observation)\n",
        "    observation, reward, done, info = env.step([action])\n",
        "    game_reward += reward\n",
        "    if render:\n",
        "        env.render()\n",
        "    pbar.update(1)\n",
        "        \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}