{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q Learning to Play CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "import gym, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "RESUME = False\n",
    "path_base = \"models/\"\n",
    "if COLAB:\n",
    "    path_base = \"drive/My Drive/\"\n",
    "env = gym.make('CartPole-v1')\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "print (env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, params):\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = params[\"decay\"]\n",
    "        self.epsilon_min = params[\"min_epsilon\"]\n",
    "        self.discount = params[\"discount\"]\n",
    "        self.merge_frequency = params[\"merge_frequency\"]\n",
    "        self.save_frequency = params[\"save_frequency\"]\n",
    "        self.num_actions = params[\"actions\"]\n",
    "        self.batch_size = params[\"batch_size\"]\n",
    "        self.optimizer = params[\"optimizer\"]\n",
    "        self.experience_memory = params[\"memory\"]\n",
    "        self.experience = deque()\n",
    "        self.count = 0\n",
    "        self.game = 0\n",
    "        self.metric_states = []\n",
    "        self.metric_outputs = []\n",
    "        self.input_shape = params[\"input_shape\"]\n",
    "        self.q_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.load_weights()\n",
    "        \n",
    "    def merge_networks(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential()\n",
    "        initializer = tf.keras.initializers.GlorotUniform()\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu', \n",
    "                        input_shape=self.input_shape, kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu', \n",
    "                         kernel_initializer=initializer))\n",
    "        model.add(tf.keras.layers.Dense(self.num_actions, activation='linear', kernel_initializer=initializer))\n",
    "        model.compile(loss='mse', optimizer= self.optimizer)\n",
    "        return model\n",
    "    \n",
    "    def agent_start(self, observation):\n",
    "        q_values = self.q_network.predict(np.array([observation]))\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon, self.epsilon_min)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values[0])\n",
    "        self.prev_state = observation\n",
    "        self.prev_action = action \n",
    "        return action\n",
    "    \n",
    "    def agent_step(self, reward, observation):\n",
    "        self.count += 1\n",
    "        \n",
    "\n",
    "        if self.game == 0:\n",
    "            if self.count % 200 == 1 and len(self.metric_states) < 5:\n",
    "                self.metric_states.append(observation)\n",
    "        q_values = self.q_network.predict(np.array([observation]))\n",
    "        relay = (self.prev_state, self.prev_action,  reward, observation, 0)\n",
    "        self.experience.append(relay)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(q_values[0])\n",
    "        self.prev_state = observation\n",
    "        self.prev_action = action \n",
    "        self.train(self.batch_size)\n",
    "        \n",
    "        return action, q_values[0][action]\n",
    "        \n",
    "    \n",
    "    def agent_end(self, reward):\n",
    "        relay = (self.prev_state, self.prev_action,  reward, self.prev_state, 1)\n",
    "        self.experience.append(relay)\n",
    "        self.game += 1\n",
    "        if len(self.experience) > self.experience_memory:\n",
    "            self.experience.popleft()\n",
    "            \n",
    "        if self.game%self.merge_frequency == 0:\n",
    "            self.merge_networks()\n",
    "            \n",
    "        if self.game%self.save_frequency == 0:\n",
    "            self.save_weights()\n",
    "        \n",
    "    def save_weights(self):\n",
    "        self.q_network.save_weights(path_base + \"q-cart.h5\")\n",
    "        self.target_network.save_weights(path_base + \"target-cart.h5\")\n",
    "        print (\"saved\")\n",
    "               \n",
    "    def load_weights(self):\n",
    "        self.q_network.load_weights(path_base + \"q-cart.h5\")\n",
    "        self.target_network.load_weights(path_base + \"target-cart.h5\")\n",
    "        \n",
    "    def train(self, count):\n",
    "        batch = random.sample(self.experience, min(count, len(self.experience)))\n",
    "        step = 0\n",
    "        input_tensor = [state for state, action, reward, future, terminated in batch]\n",
    "        output_tensor = self.q_network.predict(np.array(input_tensor))\n",
    "        future_input_tensor = [future for state, action, reward, future, terminated in batch]\n",
    "        future_out = self.target_network.predict(np.array(future_input_tensor))\n",
    "        for count, (state, action, reward, future, terminated) in enumerate(batch):\n",
    "            target = output_tensor[count]\n",
    "            updated = reward\n",
    "            if not terminated:\n",
    "                target_vals = future_out[count]\n",
    "                updated += self.discount*(target_vals[np.argmax(target)])\n",
    "                \n",
    "            target[action] = updated\n",
    "            output_tensor[count] = target \n",
    "        \n",
    "        input_tensor = np.array(input_tensor)\n",
    "        output_tensor = np.array(output_tensor)\n",
    "        self.q_network.fit(input_tensor, output_tensor, epochs=1, verbose=0)\n",
    "            \n",
    "        metric = np.average([np.amax(out) for out in \n",
    "                                         self.q_network.predict(np.array(self.metric_states))])\n",
    "        self.metric_outputs.append(metric)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "params = {\"decay\":0.999, \"batch_size\":32, \"merge_frequency\": 10, \"min_epsilon\": 0.1, \"input_shape\": env.observation_space.shape, \"save_frequency\": 10, \"discount\": 0.95,  \"actions\": 2, \"optimizer\": optimizer, \n",
    "          \"memory\": 70000}\n",
    "agent = Agent(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0ab1fa8d3f4bb2841385d818daf9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Game: ', max=400, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9443acc0d3d4d5b8505f8db02e2648e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Game Step: ', max=1, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n",
      "saved\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "ITERATIONS = 400\n",
    "np.set_printoptions(precision=3)\n",
    "render = True\n",
    "gbar = tqdm(desc=\"Game: \", total=ITERATIONS)\n",
    "pbar = tqdm(desc=\"Game Step: \")\n",
    "y = []\n",
    "x = []\n",
    "for _ in range(ITERATIONS):\n",
    "    action = agent.agent_start(env.reset())\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    game_reward = 0\n",
    "    while not done:\n",
    "        action, value = agent.agent_step(reward, observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -reward\n",
    "        game_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "        pbar.set_description(\"Action Value {0} Game Step: \".format(value))\n",
    "        pbar.update(1)\n",
    "        \n",
    "    y.append(game_reward)\n",
    "    x.append(_)\n",
    "    \n",
    "    gbar.update(1)\n",
    "    pbar.refresh()\n",
    "    pbar.reset()\n",
    "    agent.agent_end(reward)\n",
    "    \n",
    "plt.clf()\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 80)                30800     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 6)                 486       \n",
      "=================================================================\n",
      "Total params: 31,286\n",
      "Trainable params: 31,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
